<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Taroballz StudyNotes</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.taroballz.com/"/>
  <updated>2020-01-14T17:25:58.000Z</updated>
  <id>http://www.taroballz.com/</id>
  
  <author>
    <name>Taroballz</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Golang-select語句</title>
    <link href="http://www.taroballz.com/2020/01/15/Go_select/"/>
    <id>http://www.taroballz.com/2020/01/15/Go_select/</id>
    <published>2020-01-14T16:00:00.000Z</published>
    <updated>2020-01-14T17:25:58.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="初識"><a href="#初識" class="headerlink" title="初識"></a>初識</h1><ul><li><code>select</code>是golang中一種控制結構<ul><li>可透過<code>select</code>語句監聽在<code>channel</code>上的數據流動狀態</li></ul></li><li>類似於<code>switch</code>語句，但是<code>select</code>會<strong>隨機執行</strong>任何一個可運行的<code>case</code></li><li>沒有<code>case</code>可運行時，則阻塞；直到有<code>case</code>可運行</li></ul><hr><a id="more"></a><h1 id="語法結構"><a href="#語法結構" class="headerlink" title="語法結構"></a>語法結構</h1><p>類似<code>switch</code>，仍存在<code>case</code>語句和<code>default</code>語句<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> communication clause:</span><br><span class="line">        statement(s)</span><br><span class="line">    <span class="keyword">case</span> communication clase:</span><br><span class="line">        statement(s)</span><br><span class="line">    <span class="comment">//可定義任意數量的case</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">default</span>: <span class="comment">//可選擇使用</span></span><br><span class="line">        statments(s)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><ul><li>每一個<code>case</code>都是一個<strong>通信</strong><ul><li>必須為對channel操作(對channel發送數據 or 從channel中獲取數據 皆可)</li></ul></li><li>(重要)如果有多個<code>case</code>都可運行，<code>select</code>會隨機地選出一個執行，其他<code>case</code>便不會執行</li><li>否則：<ul><li>如果有<code>default</code>語句，則執行該語句</li><li>無<code>default</code>語句時，<code>select</code>將阻塞，直到某個通信可以運行</li></ul></li></ul><hr><h1 id="Example-I"><a href="#Example-I" class="headerlink" title="Example I:"></a>Example I:</h1><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line"><span class="string">"fmt"</span></span><br><span class="line"><span class="string">"time"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">timer := time.NewTimer(<span class="number">1</span>*time.Second)</span><br><span class="line">timer2 := time.NewTimer(<span class="number">2</span>*time.Second)</span><br><span class="line"><span class="keyword">for</span> i:=<span class="number">0</span>;i&lt;<span class="number">2</span>;i++&#123;</span><br><span class="line"><span class="keyword">select</span> &#123;</span><br><span class="line"><span class="keyword">case</span> &lt;-timer.C:</span><br><span class="line">fmt.Println(<span class="string">"1"</span>)</span><br><span class="line"><span class="keyword">case</span> &lt;-timer2.C:</span><br><span class="line">fmt.Println(<span class="string">"2"</span>)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><h2 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td></tr></table></figure><h1 id="Example-II"><a href="#Example-II" class="headerlink" title="Example II:"></a>Example II:</h1><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line"><span class="string">"fmt"</span></span><br><span class="line"><span class="string">"time"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">ch1 := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">int</span>,<span class="number">1</span>)</span><br><span class="line">ch2 := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">int</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span>&#123;</span><br><span class="line"><span class="keyword">for</span> &#123;</span><br><span class="line">ch2 &lt;- <span class="number">100</span></span><br><span class="line">time.Sleep(<span class="number">100</span>*time.Millisecond)</span><br><span class="line">&#125;</span><br><span class="line">&#125;()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> &#123;</span><br><span class="line"><span class="keyword">select</span> &#123;</span><br><span class="line"><span class="keyword">case</span> ch1 &lt;- <span class="number">1</span>:</span><br><span class="line">fmt.Println(<span class="string">"1"</span>)</span><br><span class="line">time.Sleep(<span class="number">1</span>*time.Second)</span><br><span class="line"><span class="keyword">case</span> num := &lt;- ch1:</span><br><span class="line">fmt.Println(<span class="string">"get data from ch1"</span>,num)</span><br><span class="line"><span class="keyword">case</span> &lt;- ch2 :</span><br><span class="line">fmt.Println(<span class="string">"2"</span>)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Result-1"><a href="#Result-1" class="headerlink" title="Result"></a>Result</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">get data from ch1 1</span><br><span class="line">1</span><br><span class="line">get data from ch1 1</span><br><span class="line">2</span><br><span class="line">1</span><br><span class="line">get data from ch1 1</span><br><span class="line">2</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">get data from ch1 1</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">get data from ch1 1</span><br><span class="line">1</span><br><span class="line">get data from ch1 1</span><br><span class="line">1</span><br><span class="line">2</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;初識&quot;&gt;&lt;a href=&quot;#初識&quot; class=&quot;headerlink&quot; title=&quot;初識&quot;&gt;&lt;/a&gt;初識&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;code&gt;select&lt;/code&gt;是golang中一種控制結構&lt;ul&gt;
&lt;li&gt;可透過&lt;code&gt;select&lt;/code&gt;語句監聽在&lt;code&gt;channel&lt;/code&gt;上的數據流動狀態&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;類似於&lt;code&gt;switch&lt;/code&gt;語句，但是&lt;code&gt;select&lt;/code&gt;會&lt;strong&gt;隨機執行&lt;/strong&gt;任何一個可運行的&lt;code&gt;case&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;沒有&lt;code&gt;case&lt;/code&gt;可運行時，則阻塞；直到有&lt;code&gt;case&lt;/code&gt;可運行&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
    
    </summary>
    
      <category term="Golang" scheme="http://www.taroballz.com/categories/Golang/"/>
    
    
      <category term="Golang" scheme="http://www.taroballz.com/tags/Golang/"/>
    
      <category term="goroutine" scheme="http://www.taroballz.com/tags/goroutine/"/>
    
      <category term="2020" scheme="http://www.taroballz.com/tags/2020/"/>
    
  </entry>
  
  <entry>
    <title>Golang多線程-線程同步(sync)包-WaitGroup</title>
    <link href="http://www.taroballz.com/2020/01/02/Go_sync_Waitgroup/"/>
    <id>http://www.taroballz.com/2020/01/02/Go_sync_Waitgroup/</id>
    <published>2020-01-01T16:00:00.000Z</published>
    <updated>2020-01-02T16:11:19.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="WaitGroup"><a href="#WaitGroup" class="headerlink" title="WaitGroup"></a><code>WaitGroup</code></h1><ul><li>其位於較低級的同步包<code>sync</code> </li><li><code>WaitGroup</code>為一個結構體；其實例化的用途為等待<strong>一組goroutine的集合</strong> 完成工作</li><li><code>WaitGroup</code>中都有一個counter(計數器)用來記錄等待goroutine的數量</li></ul><hr><a id="more"></a><h2 id="Add"><a href="#Add" class="headerlink" title="Add"></a>Add</h2><p>用來設置<code>WaitGroup</code>實例中goroutine counter的數量</p><ul><li>在執行goroutine之前先創建對象並加入欲管理的goroutine的數量</li><li>counter的值為0時代表 等待阻塞的goroutine皆已經被釋放</li><li>若是counter的值為負數，則會<code>panic</code></li></ul><hr><h2 id="Wait"><a href="#Wait" class="headerlink" title="Wait"></a>Wait</h2><p>等待其它線程完成，並進入阻塞狀態</p><ul><li>當實例化的<code>WaitGroup</code>的counter數為0時就會解除當前線程的阻塞狀態</li></ul><h2 id="Done"><a href="#Done" class="headerlink" title="Done"></a>Done</h2><p>讓實例化的<code>WaitGroup</code>對象數值減 <strong>1</strong></p><ul><li>其內部代碼就是調用了Add方法只是將其參數(<code>delta</code>)設為 <strong>-1</strong><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(wg *WaitGroup)</span> <span class="title">Done</span><span class="params">()</span></span>&#123;</span><br><span class="line">    wg.Add(<span class="number">-1</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><h1 id="Example-Code"><a href="#Example-Code" class="headerlink" title="Example Code"></a>Example Code</h1><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line"><span class="string">"fmt"</span></span><br><span class="line"><span class="string">"sync"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">//創建WaitGroup的實例對象</span></span><br><span class="line"><span class="keyword">var</span> wg sync.WaitGroup</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line"></span><br><span class="line">wg.Add(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">go</span> PrintNum()</span><br><span class="line"><span class="keyword">go</span> PrintByte()</span><br><span class="line"></span><br><span class="line">fmt.Println(<span class="string">"Main Goroutine 將進入阻塞阻塞狀態，等待子goroutine完成工作"</span>)</span><br><span class="line">wg.Wait()  <span class="comment">//main goroutine進入阻塞</span></span><br><span class="line"></span><br><span class="line">fmt.Println(<span class="string">"子goroutine工作已全數完成，main goroutine解除阻塞"</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">PrintNum</span><span class="params">()</span></span> &#123;</span><br><span class="line"><span class="keyword">for</span> i:=<span class="number">0</span>;i&lt;<span class="number">10</span>;i++ &#123;</span><br><span class="line">fmt.Println(<span class="string">"PrintNum Gorotine"</span>,i)</span><br><span class="line">&#125;</span><br><span class="line">wg.Done()  <span class="comment">//給WaitGroup的實例化對象counter數值減1，同wg.Add(-1)</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">PrintByte</span><span class="params">()</span></span> &#123;</span><br><span class="line"><span class="keyword">for</span> i:=<span class="number">0</span>;i&lt;<span class="number">10</span>;i++ &#123;</span><br><span class="line">char := <span class="keyword">byte</span>(<span class="number">65</span> + i)</span><br><span class="line">fmt.Println(<span class="keyword">string</span>(char))</span><br><span class="line">&#125;</span><br><span class="line">wg.Done()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">Main Goroutine 將進入阻塞阻塞狀態，等待子goroutine完成工作</span><br><span class="line">A</span><br><span class="line">B</span><br><span class="line">C</span><br><span class="line">D</span><br><span class="line">E</span><br><span class="line">F</span><br><span class="line">G</span><br><span class="line">H</span><br><span class="line">I</span><br><span class="line">J</span><br><span class="line">PrintNum Gorotine 0</span><br><span class="line">PrintNum Gorotine 1</span><br><span class="line">PrintNum Gorotine 2</span><br><span class="line">PrintNum Gorotine 3</span><br><span class="line">PrintNum Gorotine 4</span><br><span class="line">PrintNum Gorotine 5</span><br><span class="line">PrintNum Gorotine 6</span><br><span class="line">PrintNum Gorotine 7</span><br><span class="line">PrintNum Gorotine 8</span><br><span class="line">PrintNum Gorotine 9</span><br><span class="line">子goroutine工作已全數完成，main goroutine解除阻塞</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;WaitGroup&quot;&gt;&lt;a href=&quot;#WaitGroup&quot; class=&quot;headerlink&quot; title=&quot;WaitGroup&quot;&gt;&lt;/a&gt;&lt;code&gt;WaitGroup&lt;/code&gt;&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;其位於較低級的同步包&lt;code&gt;sync&lt;/code&gt; &lt;/li&gt;
&lt;li&gt;&lt;code&gt;WaitGroup&lt;/code&gt;為一個結構體；其實例化的用途為等待&lt;strong&gt;一組goroutine的集合&lt;/strong&gt; 完成工作&lt;/li&gt;
&lt;li&gt;&lt;code&gt;WaitGroup&lt;/code&gt;中都有一個counter(計數器)用來記錄等待goroutine的數量&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
    
    </summary>
    
      <category term="Golang" scheme="http://www.taroballz.com/categories/Golang/"/>
    
    
      <category term="Golang" scheme="http://www.taroballz.com/tags/Golang/"/>
    
      <category term="多線程" scheme="http://www.taroballz.com/tags/%E5%A4%9A%E7%B7%9A%E7%A8%8B/"/>
    
      <category term="goroutine" scheme="http://www.taroballz.com/tags/goroutine/"/>
    
      <category term="2020" scheme="http://www.taroballz.com/tags/2020/"/>
    
      <category term="線程同步(sync)" scheme="http://www.taroballz.com/tags/%E7%B7%9A%E7%A8%8B%E5%90%8C%E6%AD%A5-sync/"/>
    
  </entry>
  
  <entry>
    <title>深度學習-神經網路基礎</title>
    <link href="http://www.taroballz.com/2019/07/31/DL_NeuralNetwork_intro/"/>
    <id>http://www.taroballz.com/2019/07/31/DL_NeuralNetwork_intro/</id>
    <published>2019-07-30T16:12:00.000Z</published>
    <updated>2019-08-28T15:07:57.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><ul><li>神經網路是為了模擬神經元傳遞的過程</li><li>不同結構的神經網路解決不同的問題</li></ul><a id="more"></a><h1 id="感知器-Perceptron"><a href="#感知器-Perceptron" class="headerlink" title="感知器(Perceptron)"></a>感知器(Perceptron)</h1><p>有n個輸入數據，通過<strong>權重</strong>與各數據之間的計算和，<strong>比較激活函數結果</strong>，得出輸出</p><ul><li>應用：很容易解決與(and)、或(or)問題</li><li>常用來解決<strong>分類</strong>問題</li><li>一個感知器通常建立一條直線<ul><li>單個感知器解決不了的問題，可以<strong>增加感知機</strong>的數目</li></ul></li></ul><h2 id="結構"><a href="#結構" class="headerlink" title="結構"></a>結構</h2><p><img src="https://imgur.com/LUz7mot.png" alt="Perceptron"></p><ul><li>其中$f$為閾值(threshold)<ul><li>大於$f$或是小於$f$則屬於不同類別</li></ul></li></ul><h1 id="神經網路的特點"><a href="#神經網路的特點" class="headerlink" title="神經網路的特點"></a>神經網路的特點</h1><p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/1200px-Colored_neural_network.svg.png" alt="neral network"></p><ul><li>輸入向量的維度和輸入神經元的個數相同</li><li>每個連接都有個權值</li><li>同一層神經元之間沒有連接</li><li>由<strong>輸入層</strong>、<strong>隱層</strong>、<strong>輸出層</strong>組成</li><li>最後一層與第前一層的所有神經元連接，也叫<strong>全連接</strong><br>  其最後有多少全連接神經元，代表共有多少輸出類別</li></ul><h1 id="神經網路組成"><a href="#神經網路組成" class="headerlink" title="神經網路組成"></a>神經網路組成</h1><ul><li>結構(Architecture)：例如神經網路中權重、神經元等等</li><li>激活函數(Activity Rule)：</li><li>學習規則(Learning Rule)：學習規則指定了網路中的權重如何隨著時間推進而調整。(反向傳播算法)</li></ul><h2 id="神經網絡的多分類問題"><a href="#神經網絡的多分類問題" class="headerlink" title="神經網絡的多分類問題"></a>神經網絡的多分類問題</h2><p>輸入一個樣本，並得出這個樣本屬於全部每一個類別的概率，並比較哪個概率較大決定類別</p><ul><li>其有多少類別輸出就為多少個類別</li></ul><h1 id="神經網路API"><a href="#神經網路API" class="headerlink" title="神經網路API"></a>神經網路API</h1><p>在使用tensorflow時，<code>tf.nn</code>、<code>tf.layers</code>、<code>tf.contrib</code>模塊有很多功能式重複的</p><h2 id="tf-nn"><a href="#tf-nn" class="headerlink" title="tf.nn"></a>tf.nn</h2><p>提供神經網路相關操作的支持，包括卷積操作(conv)、池化操作(pooling)、歸一化、損失(loss)、分類操作、embedding、RNN、Evaluation</p><h2 id="tf-layers"><a href="#tf-layers" class="headerlink" title="tf.layers"></a>tf.layers</h2><p>主要提供的高層神經網路，主要和卷積相關，對<code>tf.nn</code>進一步的封裝</p><h2 id="tf-contrib-最高層的接口"><a href="#tf-contrib-最高層的接口" class="headerlink" title="tf.contrib(最高層的接口)"></a>tf.contrib(最高層的接口)</h2><p><code>tf.contrib.layers</code>提供夠將計算圖中的網絡層、正則化、摘要操作。是構建計算圖的高級操作，但是<code>tf.contrib</code>包不穩定以及擁有一些實驗性的代碼</p><h1 id="SoftMax回歸"><a href="#SoftMax回歸" class="headerlink" title="SoftMax回歸"></a>SoftMax回歸</h1><p>公式：</p><script type="math/tex; mode=display">S_i = \frac{e^i}{\sum_je^j}</script><p><img src="https://imgur.com/KwzGfg7.png" alt="softmax"></p><ul><li>其經過softmax後的結果類似邏輯回歸為一<strong>概率值</strong></li><li>所有類別的概率值相加都等於1</li><li>真實類別的值為one-hot編碼形式</li></ul><h2 id="全連接-從輸入直接到輸出"><a href="#全連接-從輸入直接到輸出" class="headerlink" title="全連接-從輸入直接到輸出"></a>全連接-從輸入直接到輸出</h2><ul><li>特徵加權：<ul><li><code>tf.matmul(a, b, name=None)</code> + $bias$<ul><li>return: 全連接的結果，供交叉損失運算</li><li>不需要激活函數(因為是最後的輸出)</li></ul></li></ul></li></ul><h1 id="神經網絡策略-交叉熵損失"><a href="#神經網絡策略-交叉熵損失" class="headerlink" title="神經網絡策略-交叉熵損失"></a>神經網絡策略-交叉熵損失</h1><p>類似邏輯回歸-對數似然損失的推廣</p><script type="math/tex; mode=display">H_{y'}(y) = - \sum_{i}y_{i}'log(y_i)</script><ul><li>$y_{i}’$ 為真實結果，$y_i$為softmax後結果</li><li>一個樣本就有一個交叉熵損失</li><li>softmax計算出來的概率值 越接近真實值類別onehot編碼的話 表示損失越小</li></ul><h2 id="SoftMax計算、交叉熵API"><a href="#SoftMax計算、交叉熵API" class="headerlink" title="SoftMax計算、交叉熵API"></a>SoftMax計算、交叉熵API</h2><p>求所有樣本的損失，然後求平均損失</p><ul><li><p><code>tf.nn.softmax_cross_entropy_with_logits(labels=None,logits=None, name=None)</code></p><ul><li>計算logits和labels之間的交叉損失熵</li><li><code>labels</code>:標籤值(真實值)</li><li><code>logits</code>:樣本加權之後的值(預測值)</li><li>return:返回損失列表</li></ul></li><li><p><code>tf.reduce_mean(input_tensor)</code></p><ul><li>計算張量的尺寸的元素平均值</li></ul></li></ul><h1 id="神經網絡優化-反向傳播算法"><a href="#神經網絡優化-反向傳播算法" class="headerlink" title="神經網絡優化-反向傳播算法"></a>神經網絡優化-反向傳播算法</h1><p>就是梯度下降</p><h2 id="損失下降API"><a href="#損失下降API" class="headerlink" title="損失下降API"></a>損失下降API</h2><p><code>tf.train.GradientDescentOptimizer(learning_rate)</code></p><ul><li>用於梯度下降優化</li><li><code>learning_rate</code>:學習率</li><li><code>minimize(loss)</code>：最小化損失</li><li>return:梯度下降op</li></ul><h1 id="準確率計算API"><a href="#準確率計算API" class="headerlink" title="準確率計算API"></a>準確率計算API</h1><ol><li><p><code>equal_list = tf.equal(tf.argmax(y,1),tf.argmax(y_label,1))</code></p><ul><li>找尋特徵得到概率最大值的index，如果與onehot編碼label為1的index相同則為1，不同則為0</li><li><code>tf.argmax(data,row or column)</code><ul><li>返回data中的最大值index</li><li>第二個參數為0代表取列中最大值的索引，1則為行中最大值的索引</li></ul></li></ul></li><li><p><code>accuracy = tf.reduce_mean(tf.cast(equal_list, tf.float32))</code> </p></li></ol><h1 id="mnist-dataset的基本操作"><a href="#mnist-dataset的基本操作" class="headerlink" title="mnist dataset的基本操作"></a>mnist dataset的基本操作</h1><p><a href="https://github.com/curtis992250/MachineLearning_StudyNote/blob/master/mnist.ipynb" target="_blank" rel="noopener">https://github.com/curtis992250/MachineLearning_StudyNote/blob/master/mnist.ipynb</a></p><h1 id="Example-單層（全連接層）實現手寫數字識別"><a href="#Example-單層（全連接層）實現手寫數字識別" class="headerlink" title="Example 單層（全連接層）實現手寫數字識別"></a>Example 單層（全連接層）實現手寫數字識別</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定義命令行參數</span></span><br><span class="line">FLAGS = tf.app.flags.FLAGS</span><br><span class="line"></span><br><span class="line">tf.app.flags.DEFINE_integer(<span class="string">"is_train"</span>,<span class="number">1</span>,<span class="string">"指定程序為預測或是訓練模型"</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">full_connected</span><span class="params">()</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#獲取真實的數據</span></span><br><span class="line">    mnist = input_data.read_data_sets(<span class="string">"./Training_Data/mnistData"</span>, one_hot=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 1. 建立數據的佔位符 x[None, 781] y_true[None, 10]</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"data"</span>):</span><br><span class="line">        x = tf.placeholder(tf.float32, [<span class="literal">None</span>,<span class="number">784</span>])</span><br><span class="line">        y_true = tf.placeholder(tf.int32, [<span class="literal">None</span>,<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2.建立一個全連接層的神經網絡 weight[784,10] bias[10]</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"full_connected_model"</span>):</span><br><span class="line">        <span class="comment"># 隨機初始化權重和偏置</span></span><br><span class="line">        weight = tf.Variable(tf.random_normal([<span class="number">784</span>,<span class="number">10</span>],mean=<span class="number">0.0</span>, stddev=<span class="number">1.0</span>,name=<span class="string">"weight"</span>))</span><br><span class="line"></span><br><span class="line">        bias = tf.Variable(tf.constant(<span class="number">0.0</span>,shape=[<span class="number">10</span>]))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 預測None個樣本的輸出結果 x[None, 784] * w[784, 10] + b[10] = result[None,10]</span></span><br><span class="line">        y_predict = tf.matmul(x, weight) + bias</span><br><span class="line"></span><br><span class="line">    <span class="comment">#3. 求出所有樣本的損失，然後求平均值</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"soft_cross"</span>):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 求平均交叉熵損失</span></span><br><span class="line">        loss_list = tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=y_predict)</span><br><span class="line">        loss = tf.reduce_mean(loss_list)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 4. 梯度下降求出損失</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"optimizer"</span>):</span><br><span class="line">        train_op = tf.train.GradientDescentOptimizer(learning_rate=<span class="number">0.1</span>).minimize(loss=loss)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 5. 計算準確率</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"calc_accuracy"</span>):</span><br><span class="line"></span><br><span class="line">        equal_list = tf.equal(tf.arg_max(y_true,<span class="number">1</span>), tf.arg_max(y_predict,<span class="number">1</span>))</span><br><span class="line">        <span class="comment"># equal_list 應有None個樣本類似 [0,1,0,0,1,1,1,1,....]</span></span><br><span class="line"></span><br><span class="line">        accuracy = tf.reduce_mean(tf.cast(equal_list, tf.float32))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 收集單個數字的變量</span></span><br><span class="line">    tf.summary.scalar(<span class="string">"losses"</span>, loss)</span><br><span class="line">    tf.summary.scalar(<span class="string">"accuracy"</span>, accuracy)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 收集高維度的變量</span></span><br><span class="line">    tf.summary.histogram(<span class="string">"weights"</span>, weight)</span><br><span class="line">    tf.summary.histogram(<span class="string">"biases"</span>,bias)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定義一個初始化變量的op</span></span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定義一個合併變量的op</span></span><br><span class="line">    merged = tf.summary.merge_all()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 創建一個saver用於保存模型(Save, restore方法)</span></span><br><span class="line">    saver = tf.train.Saver()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 開啟會話訓練</span></span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        <span class="comment">#初始化變量</span></span><br><span class="line">        sess.run(init_op)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 建立events文件然後寫入</span></span><br><span class="line">        filewriter = tf.summary.FileWriter(<span class="string">"./summary/number_recognize"</span>,graph=sess.graph)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> FLAGS.is_train == <span class="literal">True</span>:</span><br><span class="line"></span><br><span class="line">            <span class="comment">#迭代步數去訓練，更新參數預測</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2000</span>):</span><br><span class="line"></span><br><span class="line">                <span class="comment">#取出真實存在的特徵值和目標值</span></span><br><span class="line">                mnist_x, mnist_y = mnist.train.next_batch(<span class="number">50</span>)</span><br><span class="line"></span><br><span class="line">                <span class="comment">#運行train_op訓練</span></span><br><span class="line">                sess.run(train_op, feed_dict=&#123;x: mnist_x,y_true: mnist_y&#125;)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 寫入每步訓練的值</span></span><br><span class="line">                summary = sess.run(merged, feed_dict=&#123;x: mnist_x,y_true: mnist_y&#125;)</span><br><span class="line"></span><br><span class="line">                filewriter.add_summary(summary,i)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">                print(<span class="string">"訓練第%d步，準確率為：%f"</span>%(i,sess.run(accuracy, feed_dict=&#123;x: mnist_x,y_true: mnist_y&#125;)))</span><br><span class="line"></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># 訓練結束後保存模型</span></span><br><span class="line">                saver.save(sess,<span class="string">"./ckpt/fc_model"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 加載模型</span></span><br><span class="line">            saver.restore(sess,<span class="string">"./ckpt/fc_model"</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 要是is_train為則做出預測</span></span><br><span class="line">            <span class="keyword">for</span>  i <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line"></span><br><span class="line">                <span class="comment">#每次測試一張圖片[0,0,0,0,0,0,1,0,0,0]</span></span><br><span class="line">                x_test, y_test = mnist.test.next_batch(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">                print(<span class="string">"第%d張圖片，手寫數字圖片為%d，預測結果是:%d"</span>%(i,</span><br><span class="line">                                                               tf.argmax(y_test,<span class="number">1</span>).eval(),</span><br><span class="line">                                                               tf.argmax(sess.run(y_predict,feed_dict=&#123;x: x_test,y_true: y_test&#125;),<span class="number">1</span>).eval()</span><br><span class="line">                                                   ))</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    full_connected()</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;神經網路是為了模擬神經元傳遞的過程&lt;/li&gt;
&lt;li&gt;不同結構的神經網路解決不同的問題&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="tensorflow深度學習" scheme="http://www.taroballz.com/categories/tensorflow%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
      <category term="Python" scheme="http://www.taroballz.com/tags/Python/"/>
    
      <category term="tensorflow" scheme="http://www.taroballz.com/tags/tensorflow/"/>
    
      <category term="2019" scheme="http://www.taroballz.com/tags/2019/"/>
    
      <category term="深度學習(Deep Learning)" scheme="http://www.taroballz.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92-Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>深度學習-tensorflow基礎-讀取數據-API應用-二進制文件讀取</title>
    <link href="http://www.taroballz.com/2019/07/29/DL_BinaryReadAPI/"/>
    <id>http://www.taroballz.com/2019/07/29/DL_BinaryReadAPI/</id>
    <published>2019-07-29T14:16:00.000Z</published>
    <updated>2019-07-30T15:29:20.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><ul><li>圖片也算是一種二進制文件格式</li><li>將進行<code>CIFAR-10</code>二進制數據讀取<ul><li><a href="https://www.cs.toronto.edu/~kriz/cifar.html" target="_blank" rel="noopener">https://www.cs.toronto.edu/~kriz/cifar.html</a></li><li>全部共60000萬張圖片，每張圖片為32x32彩色圖像所組成<ul><li>50000個訓練集及10000個測試集</li><li>五個訓練批次(<code>data_batch_1.bin</code>, <code>data_batch_2.bin</code>…)</li><li>一個測試批次(<code>test_batch.bin</code>)</li></ul></li><li>其有10個類別(飛機、汽車、鳥…等)的圖片<ul><li>每個類別共6000個圖片</li></ul></li><li>文件中的格式都是第1個像素為標籤(0~9) 剩下的 3072 (3*32*32)像素為特徵<ul><li>前1024像素為red_channel, 下1024像素為green_channel, 最後1024像素為blue_channel</li></ul></li></ul></li></ul><a id="more"></a><h1 id="構造圖片文件隊列"><a href="#構造圖片文件隊列" class="headerlink" title="構造圖片文件隊列"></a>構造圖片文件隊列</h1><p>與讀取文字文件的文件隊列相同</p><h1 id="構造讀取器"><a href="#構造讀取器" class="headerlink" title="構造讀取器"></a>構造讀取器</h1><ul><li>讀取每個記錄是<strong>固定數量bytes</strong>的二進制文件</li><li><code>tf.FixedLengthRecordReader(record_bytes)</code><ul><li><code>record_bytes</code>:整型參數，指定每次讀取(一個樣本)為多少bytes</li><li>返回一個讀取器實例</li></ul></li></ul><h2 id="method"><a href="#method" class="headerlink" title="method"></a>method</h2><ul><li><code>read(file_queue)</code>：輸出將是一個<strong>文件名(key)</strong>和該<strong>文件的內容(value)</strong></li></ul><h1 id="二進制文件解碼"><a href="#二進制文件解碼" class="headerlink" title="二進制文件解碼"></a>二進制文件解碼</h1><p><code>tf.decode_raw(bytes, out_type, little_endian=None, name=None)</code></p><ul><li><code>bytes</code>:欲解碼的數據，也就是<code>read</code>方法返回的<code>value</code></li><li>將bytes轉換為一個數字向量表示<ul><li>bytes原本為一字符串類型的張量</li></ul></li><li>與函數<code>tf.FixedLengthRecordReader()</code>搭配使用</li><li>讀取後優先為<code>uint8</code>類型，可改變<code>out_type</code>參數改變成不同類型</li><li><strong>其解碼出來的<code>shape</code>並不是固定</strong>，所以須自己額外去固定形狀</li></ul><h1 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定義cifar數據等命令行參數</span></span><br><span class="line">FLAGS = tf.app.flags.FLAGS</span><br><span class="line"></span><br><span class="line">tf.app.flags.DEFINE_string(<span class="string">"cifar_dir"</span>,<span class="string">"./cifar-10-batches-bin/"</span>,<span class="string">"where's cifar data dir?"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CifarRead</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,filelist)</span>:</span></span><br><span class="line">        <span class="comment"># 文件列表</span></span><br><span class="line">        self.filelist = filelist</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定義讀取圖片的屬性</span></span><br><span class="line">        self.width = <span class="number">32</span></span><br><span class="line">        self.height = <span class="number">32</span></span><br><span class="line">        self.channel = <span class="number">3</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 二進制文件圖片的bytes</span></span><br><span class="line">        self.label_pixel = <span class="number">1</span>  <span class="comment"># 標籤的bytes</span></span><br><span class="line">        self.image_pixel = self.width * self.height * self.channel</span><br><span class="line">        self.whole_image = self.label_pixel + self.image_pixel</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">read_and_decode</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># 1. 構造文件名隊列</span></span><br><span class="line">        file_queue = tf.train.string_input_producer(self.filelist)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2. 構造二進制文件讀取器，讀取內容，cifar-10每個樣本為3073bytes</span></span><br><span class="line">        binary_reader = tf.FixedLengthRecordReader(record_bytes=self.whole_image)</span><br><span class="line"></span><br><span class="line">        key, value = binary_reader.read(file_queue)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 3. 解碼內容（進行二進制文件內容的解碼）</span></span><br><span class="line">        label_image = tf.decode_raw(value, out_type=tf.uint8)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 4. 分割圖片和標籤數據，切出特徵值及目標值</span></span><br><span class="line">        label = tf.slice(label_image, [<span class="number">0</span>], [self.label_pixel])</span><br><span class="line">        image = tf.slice(label_image, [self.label_pixel], [self.image_pixel])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 將label轉換為int32類型</span></span><br><span class="line">        label = tf.cast(label,tf.int32)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 5. 對圖片的特徵數據進行形狀的改變 [3072] ---&gt; [3, 32, 32]</span></span><br><span class="line">        image_reshape = tf.reshape(image,[self.channel, self.height, self.width ])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 6. 批處理數據</span></span><br><span class="line">        image_batch, label_batch = tf.train.batch([image_reshape, label], batch_size=<span class="number">10</span>, num_threads=<span class="number">1</span>, capacity=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">        print(image_batch, label_batch)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> image_batch, label_batch</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="comment"># 1. 找到文件，放入列表   路徑+文件名字 --&gt; 列表當中</span></span><br><span class="line">    file_name = os.listdir(FLAGS.cifar_dir)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 對二進制文件來說我們只要裡面的bin文件</span></span><br><span class="line">    file_list = [os.path.join(FLAGS.cifar_dir, file) <span class="keyword">for</span> file <span class="keyword">in</span> file_name <span class="keyword">if</span> file.endswith(<span class="string">'bin'</span>)]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 實例化我們自定義CifarRead實例用於讀取使用</span></span><br><span class="line">    cfR = CifarRead(file_list)</span><br><span class="line"></span><br><span class="line">    image_batch, label_batch = cfR.read_and_decode()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        <span class="comment"># 定義一個線程協調器</span></span><br><span class="line">        coord = tf.train.Coordinator()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 開啟讀文件的線程</span></span><br><span class="line">        threads = tf.train.start_queue_runners(sess, coord=coord)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 顯示讀取內容</span></span><br><span class="line">        print(sess.run([image_batch, label_batch]))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 回收子線程</span></span><br><span class="line">        coord.request_stop()</span><br><span class="line"></span><br><span class="line">        coord.join(threads)</span><br></pre></td></tr></table></figure><h2 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br></pre></td><td class="code"><pre><span class="line">Tensor(&quot;batch:0&quot;, shape=(10, 32, 32, 3), dtype=uint8) Tensor(&quot;batch:1&quot;, shape=(10, 1), dtype=int32)</span><br><span class="line">[array([[[[ 26,  17,  13],</span><br><span class="line">         [ 13,  13,  14],</span><br><span class="line">         [ 14,  15,  14],</span><br><span class="line">         ...,</span><br><span class="line">         [ 12,  15,  21],</span><br><span class="line">         [ 36,  26,  22],</span><br><span class="line">         [ 17,  25,  31]],</span><br><span class="line"></span><br><span class="line">        [[ 13,  17,  14],</span><br><span class="line">         [ 14,  11,   9],</span><br><span class="line">         [ 19,  18,  11],</span><br><span class="line">         ...,</span><br><span class="line">         [174, 229, 249],</span><br><span class="line">         [251, 244, 248],</span><br><span class="line">         [175,  29,  11]],</span><br><span class="line"></span><br><span class="line">        [[ 17,  17,  16],</span><br><span class="line">         [ 14,  12,  11],</span><br><span class="line">         [ 10,  11,  12],</span><br><span class="line">         ...,</span><br><span class="line">         [251, 252, 252],</span><br><span class="line">         [242, 240, 160],</span><br><span class="line">         [ 18,  14,  16]],</span><br><span class="line"></span><br><span class="line">        ...,</span><br><span class="line"></span><br><span class="line">        [[105,  40,  58],</span><br><span class="line">         [ 79,  79,  84],</span><br><span class="line">         [ 47,   5,   0],</span><br><span class="line">         ...,</span><br><span class="line">         [ 69,  20,  21],</span><br><span class="line">         [ 21,  20,  23],</span><br><span class="line">         [ 26,  29,  25]],</span><br><span class="line"></span><br><span class="line">        [[139, 168,  57],</span><br><span class="line">         [  4,   8,  21],</span><br><span class="line">         [ 34,  72, 105],</span><br><span class="line">         ...,</span><br><span class="line">         [142,  29,  20],</span><br><span class="line">         [ 21,  25,  25],</span><br><span class="line">         [ 44,  34,  25]],</span><br><span class="line"></span><br><span class="line">        [[137,  95,  90],</span><br><span class="line">         [ 57,  46,  55],</span><br><span class="line">         [ 28,  64, 126],</span><br><span class="line">         ...,</span><br><span class="line">         [237, 144,  33],</span><br><span class="line">         [ 29,  46,  28],</span><br><span class="line">         [ 27,  26,  27]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">       [[[ 94, 101,  95],</span><br><span class="line">         [ 94,  94,  97],</span><br><span class="line">         [111, 142, 166],</span><br><span class="line">         ...,</span><br><span class="line">         [133, 136, 138],</span><br><span class="line">         [140, 142, 145],</span><br><span class="line">         [146, 147, 123]],</span><br><span class="line"></span><br><span class="line">        [[ 84,  88, 101],</span><br><span class="line">         [102, 100,  99],</span><br><span class="line">         [109, 126, 146],</span><br><span class="line">         ...,</span><br><span class="line">         [154, 132, 124],</span><br><span class="line">         [135, 139, 141],</span><br><span class="line">         [144, 145, 121]],</span><br><span class="line"></span><br><span class="line">        [[ 90,  85,  85],</span><br><span class="line">         [119, 167, 218],</span><br><span class="line">         [224, 219, 207],</span><br><span class="line">         ...,</span><br><span class="line">         [204, 189, 161],</span><br><span class="line">         [113, 107, 117],</span><br><span class="line">         [137, 137, 115]],</span><br><span class="line"></span><br><span class="line">        ...,</span><br><span class="line"></span><br><span class="line">        [[193, 191, 178],</span><br><span class="line">         [167, 165, 170],</span><br><span class="line">         [176, 185, 193],</span><br><span class="line">         ...,</span><br><span class="line">         [181, 176, 175],</span><br><span class="line">         [182, 181, 174],</span><br><span class="line">         [160, 140, 117]],</span><br><span class="line"></span><br><span class="line">        [[197, 198, 190],</span><br><span class="line">         [180, 178, 182],</span><br><span class="line">         [183, 181, 182],</span><br><span class="line">         ...,</span><br><span class="line">         [175, 169, 163],</span><br><span class="line">         [161, 151, 144],</span><br><span class="line">         [141, 142, 117]],</span><br><span class="line"></span><br><span class="line">        [[204, 207, 200],</span><br><span class="line">         [185, 177, 178],</span><br><span class="line">         [181, 189, 195],</span><br><span class="line">         ...,</span><br><span class="line">         [145, 147, 150],</span><br><span class="line">         [152, 163, 174],</span><br><span class="line">         [182, 184, 155]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">       [[[183, 158, 166],</span><br><span class="line">         [167, 169, 171],</span><br><span class="line">         [163, 163, 160],</span><br><span class="line">         ...,</span><br><span class="line">         [ 69,  61,  61],</span><br><span class="line">         [ 57,  63,  75],</span><br><span class="line">         [ 63,  53,  60]],</span><br><span class="line"></span><br><span class="line">        [[119,  86,  82],</span><br><span class="line">         [132, 225, 181],</span><br><span class="line">         [ 90,  76,  91],</span><br><span class="line">         ...,</span><br><span class="line">         [ 59,  67,  75],</span><br><span class="line">         [ 67,  55,  54],</span><br><span class="line">         [ 59,  48,  46]],</span><br><span class="line"></span><br><span class="line">        [[103,  74,  91],</span><br><span class="line">         [161, 225, 230],</span><br><span class="line">         [170, 108,  99],</span><br><span class="line">         ...,</span><br><span class="line">         [ 62,  70,  80],</span><br><span class="line">         [ 65,  84, 115],</span><br><span class="line">         [143, 166, 148]],</span><br><span class="line"></span><br><span class="line">        ...,</span><br><span class="line"></span><br><span class="line">        [[105,  75,  91],</span><br><span class="line">         [ 99,  79,  77],</span><br><span class="line">         [ 81,  76,  85],</span><br><span class="line">         ...,</span><br><span class="line">         [ 86,  82,  76],</span><br><span class="line">         [ 65,  61,  67],</span><br><span class="line">         [ 57,  56,  67]],</span><br><span class="line"></span><br><span class="line">        [[136,  97,  76],</span><br><span class="line">         [ 79,  80,  75],</span><br><span class="line">         [ 75,  76,  76],</span><br><span class="line">         ...,</span><br><span class="line">         [ 74,  68,  70],</span><br><span class="line">         [ 81,  94,  82],</span><br><span class="line">         [ 77,  62,  78]],</span><br><span class="line"></span><br><span class="line">        [[103,  87, 105],</span><br><span class="line">         [107, 111, 103],</span><br><span class="line">         [ 94,  84, 104],</span><br><span class="line">         ...,</span><br><span class="line">         [174, 177, 227],</span><br><span class="line">         [250, 250, 250],</span><br><span class="line">         [250, 250, 250]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">       ...,</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">       [[[225, 214, 190],</span><br><span class="line">         [167, 169, 184],</span><br><span class="line">         [231, 254, 218],</span><br><span class="line">         ...,</span><br><span class="line">         [160, 163, 167],</span><br><span class="line">         [165, 161, 157],</span><br><span class="line">         [154, 153, 157]],</span><br><span class="line"></span><br><span class="line">        [[123, 129, 118],</span><br><span class="line">         [ 78,  91, 166],</span><br><span class="line">         [170, 142, 125],</span><br><span class="line">         ...,</span><br><span class="line">         [167, 171, 179],</span><br><span class="line">         [185, 175, 169],</span><br><span class="line">         [167, 165, 165]],</span><br><span class="line"></span><br><span class="line">        [[ 48,  63, 125],</span><br><span class="line">         [159, 109, 112],</span><br><span class="line">         [167, 167, 149],</span><br><span class="line">         ...,</span><br><span class="line">         [169, 177, 181],</span><br><span class="line">         [192, 196, 185],</span><br><span class="line">         [180, 174, 178]],</span><br><span class="line"></span><br><span class="line">        ...,</span><br><span class="line"></span><br><span class="line">        [[ 50,  28,  24],</span><br><span class="line">         [ 17,  13,  18],</span><br><span class="line">         [ 34,  71,  95],</span><br><span class="line">         ...,</span><br><span class="line">         [140, 125, 153],</span><br><span class="line">         [181, 159, 154],</span><br><span class="line">         [182, 193, 195]],</span><br><span class="line"></span><br><span class="line">        [[ 39,  37,  46],</span><br><span class="line">         [ 50,  49,  45],</span><br><span class="line">         [ 45,  55,  61],</span><br><span class="line">         ...,</span><br><span class="line">         [127, 123, 119],</span><br><span class="line">         [120, 123, 125],</span><br><span class="line">         [121, 114, 114]],</span><br><span class="line"></span><br><span class="line">        [[117, 115, 113],</span><br><span class="line">         [115, 125, 128],</span><br><span class="line">         [126, 128, 132],</span><br><span class="line">         ...,</span><br><span class="line">         [123, 122, 117],</span><br><span class="line">         [114, 120, 128],</span><br><span class="line">         [144, 167, 171]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">       [[[ 82,  69,  63],</span><br><span class="line">         [ 65,  68,  65],</span><br><span class="line">         [ 58,  54,  51],</span><br><span class="line">         ...,</span><br><span class="line">         [ 53,  54,  39],</span><br><span class="line">         [ 49,  68,  70],</span><br><span class="line">         [ 69,  77,  83]],</span><br><span class="line"></span><br><span class="line">        [[ 76,  74,  59],</span><br><span class="line">         [ 53,  56,  61],</span><br><span class="line">         [ 61,  57,  50],</span><br><span class="line">         ...,</span><br><span class="line">         [ 39,  54,  56],</span><br><span class="line">         [ 42,  50,  65],</span><br><span class="line">         [ 73,  78,  83]],</span><br><span class="line"></span><br><span class="line">        [[ 54,  63,  66],</span><br><span class="line">         [ 56,  54,  56],</span><br><span class="line">         [ 55,  56,  66],</span><br><span class="line">         ...,</span><br><span class="line">         [ 33,  44,  54],</span><br><span class="line">         [ 61,  55,  59],</span><br><span class="line">         [ 51,  49,  70]],</span><br><span class="line"></span><br><span class="line">        ...,</span><br><span class="line"></span><br><span class="line">        [[ 73,  85,  87],</span><br><span class="line">         [ 78,  71,  79],</span><br><span class="line">         [103, 126, 124],</span><br><span class="line">         ...,</span><br><span class="line">         [ 78,  65,  72],</span><br><span class="line">         [ 71,  57,  61],</span><br><span class="line">         [ 79,  65,  73]],</span><br><span class="line"></span><br><span class="line">        [[ 61,  47,  76],</span><br><span class="line">         [ 85,  88,  68],</span><br><span class="line">         [ 82, 105,  88],</span><br><span class="line">         ...,</span><br><span class="line">         [ 27,  40,  60],</span><br><span class="line">         [ 62,  62,  78],</span><br><span class="line">         [ 67,  61,  64]],</span><br><span class="line"></span><br><span class="line">        [[ 81,  70,  65],</span><br><span class="line">         [ 66,  85,  96],</span><br><span class="line">         [ 91, 110, 118],</span><br><span class="line">         ...,</span><br><span class="line">         [ 55,  69,  82],</span><br><span class="line">         [ 84,  80,  78],</span><br><span class="line">         [ 67,  57,  68]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">       [[[198, 173, 144],</span><br><span class="line">         [124,  96,  58],</span><br><span class="line">         [ 52,  56,  52],</span><br><span class="line">         ...,</span><br><span class="line">         [ 47,  54,  56],</span><br><span class="line">         [ 53,  51,  46],</span><br><span class="line">         [ 46,  40,  45]],</span><br><span class="line"></span><br><span class="line">        [[185, 158, 134],</span><br><span class="line">         [124, 100,  55],</span><br><span class="line">         [ 49,  47,  46],</span><br><span class="line">         ...,</span><br><span class="line">         [ 41,  40,  41],</span><br><span class="line">         [ 38,  37,  43],</span><br><span class="line">         [ 55,  58,  53]],</span><br><span class="line"></span><br><span class="line">        [[172, 151, 126],</span><br><span class="line">         [114,  88,  51],</span><br><span class="line">         [ 48,  48,  50],</span><br><span class="line">         ...,</span><br><span class="line">         [ 42,  43,  42],</span><br><span class="line">         [ 46,  45,  40],</span><br><span class="line">         [ 43,  41,  37]],</span><br><span class="line"></span><br><span class="line">        ...,</span><br><span class="line"></span><br><span class="line">        [[142, 120, 108],</span><br><span class="line">         [102,  74,  46],</span><br><span class="line">         [ 44,  45,  44],</span><br><span class="line">         ...,</span><br><span class="line">         [ 56,  39,  34],</span><br><span class="line">         [ 30,  33,  51],</span><br><span class="line">         [ 61,  52,  42]],</span><br><span class="line"></span><br><span class="line">        [[146, 121, 107],</span><br><span class="line">         [ 94,  68,  44],</span><br><span class="line">         [ 39,  37,  39],</span><br><span class="line">         ...,</span><br><span class="line">         [ 65,  55,  42],</span><br><span class="line">         [ 56,  51,  43],</span><br><span class="line">         [ 58,  57,  44]],</span><br><span class="line"></span><br><span class="line">        [[156, 126,  92],</span><br><span class="line">         [ 64,  60,  41],</span><br><span class="line">         [ 37,  36,  35],</span><br><span class="line">         ...,</span><br><span class="line">         [ 40,  45,  46],</span><br><span class="line">         [ 49,  50,  49],</span><br><span class="line">         [ 40,  31,  26]]]], dtype=uint8), array([[8],</span><br><span class="line">       [5],</span><br><span class="line">       [0],</span><br><span class="line">       [6],</span><br><span class="line">       [9],</span><br><span class="line">       [2],</span><br><span class="line">       [8],</span><br><span class="line">       [3],</span><br><span class="line">       [6],</span><br><span class="line">       [2]], dtype=int32)]</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;圖片也算是一種二進制文件格式&lt;/li&gt;
&lt;li&gt;將進行&lt;code&gt;CIFAR-10&lt;/code&gt;二進制數據讀取&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://www.cs.toronto.edu/~kriz/cifar.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.cs.toronto.edu/~kriz/cifar.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;全部共60000萬張圖片，每張圖片為32x32彩色圖像所組成&lt;ul&gt;
&lt;li&gt;50000個訓練集及10000個測試集&lt;/li&gt;
&lt;li&gt;五個訓練批次(&lt;code&gt;data_batch_1.bin&lt;/code&gt;, &lt;code&gt;data_batch_2.bin&lt;/code&gt;…)&lt;/li&gt;
&lt;li&gt;一個測試批次(&lt;code&gt;test_batch.bin&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;其有10個類別(飛機、汽車、鳥…等)的圖片&lt;ul&gt;
&lt;li&gt;每個類別共6000個圖片&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;文件中的格式都是第1個像素為標籤(0~9) 剩下的 3072 (3*32*32)像素為特徵&lt;ul&gt;
&lt;li&gt;前1024像素為red_channel, 下1024像素為green_channel, 最後1024像素為blue_channel&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="tensorflow深度學習" scheme="http://www.taroballz.com/categories/tensorflow%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
      <category term="Python" scheme="http://www.taroballz.com/tags/Python/"/>
    
      <category term="tensorflow" scheme="http://www.taroballz.com/tags/tensorflow/"/>
    
      <category term="2019" scheme="http://www.taroballz.com/tags/2019/"/>
    
      <category term="深度學習(Deep Learning)" scheme="http://www.taroballz.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92-Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>機器學習-樣本不均衡</title>
    <link href="http://www.taroballz.com/2019/07/04/ML_sample_notbalance/"/>
    <id>http://www.taroballz.com/2019/07/04/ML_sample_notbalance/</id>
    <published>2019-07-04T12:06:10.000Z</published>
    <updated>2019-07-04T12:24:03.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>樣本不均衡代表在某一類數據集中，Label其中一類佔有很大的比例</p><ul><li>通常會使用上採樣來解決問題<ul><li>將較少類的樣本增加到與較多類的樣本一樣多</li></ul></li></ul><a id="more"></a><h1 id="imblearn"><a href="#imblearn" class="headerlink" title="imblearn"></a>imblearn</h1><p><code>imblearn</code>是專門用來處理不平衡數據集的庫，性能較<code>sklearn</code>高</p><ul><li>也需使用<code>fit_sample</code>方法進行擬合，用法與<code>sklearn</code>相近<h2 id="安裝"><a href="#安裝" class="headerlink" title="安裝"></a>安裝</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install -c glemaitre imbalanced-learn</span><br></pre></td></tr></table></figure></li></ul><p>or</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install imblearn</span><br></pre></td></tr></table></figure><h2 id="上採樣釋例"><a href="#上採樣釋例" class="headerlink" title="上採樣釋例"></a>上採樣釋例</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from imblearn.over_sampling iimport SMOTE</span><br><span class="line"></span><br><span class="line">sm = SMOTE(random_state=42) #實例化</span><br><span class="line">X,y = sm.fit_sample(X,y)    #進行上採樣的轉換</span><br></pre></td></tr></table></figure><ul><li><code>X,y</code> 可為<code>pandas.DataFrame</code> 或是 <code>pandas.Series</code> 類型</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h1&gt;&lt;p&gt;樣本不均衡代表在某一類數據集中，Label其中一類佔有很大的比例&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;通常會使用上採樣來解決問題&lt;ul&gt;
&lt;li&gt;將較少類的樣本增加到與較多類的樣本一樣多&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Python機器學習" scheme="http://www.taroballz.com/categories/Python%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92/"/>
    
    
      <category term="Python" scheme="http://www.taroballz.com/tags/Python/"/>
    
      <category term="2019" scheme="http://www.taroballz.com/tags/2019/"/>
    
      <category term="機器學習(Machine Learning)" scheme="http://www.taroballz.com/tags/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>機器學習-邏輯回歸的特徵工程</title>
    <link href="http://www.taroballz.com/2019/06/26/ML_FeatureEnginering_for_LogisticRegression/"/>
    <id>http://www.taroballz.com/2019/06/26/ML_FeatureEnginering_for_LogisticRegression/</id>
    <published>2019-06-25T16:37:00.000Z</published>
    <updated>2019-06-26T15:03:53.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><ul><li>在進行邏輯回歸模型訓練時，通常需要對數據進行降維</li><li>在高維(&gt;1000個)特徵情況下，可先使用特徵選擇的演算法幫助我們篩選特徵，在根據自己相關的業務常識選擇更為相關的特徵</li><li>在邏輯回歸中，<strong>一般不使用PCA和SVD的降維演算法</strong><ul><li>會抹滅特徵的可解釋性<ul><li>降維後會無法解釋特徵與Label的關係</li></ul></li><li>在不需探究特徵與標籤之間關係的線性數據上，仍可使用<ul><li>著重於對測試樣本進行分類時，可使用</li></ul></li></ul></li><li>在邏輯回歸中，前幾章提到的特徵選擇方法都可以使用<a id="more"></a></li></ul><hr><h1 id="嵌入法"><a href="#嵌入法" class="headerlink" title="嵌入法"></a>嵌入法</h1><ul><li>L1 正則化會使得部分特徵參數(權重)$\theta$($w$)變為0，因此L1正則化可以用來做特徵選擇</li><li>結合特徵選擇-嵌入法模塊(<code>sklearn.feature_selection.SelectFromModel</code>)</li><li>邏輯回歸預設是使用 <strong>L1,L2懲罰項</strong>來進行特徵篩選的</li><li>邏輯回歸具有<code>coef_</code>屬性，指定<code>norm_order</code>參數，來決定使用范數進行降維<ul><li>預設為<code>1</code> 為L1范數</li></ul></li><li>選擇器會刪除在L1範式下無效的特徵</li></ul><h2 id="優化"><a href="#優化" class="headerlink" title="優化"></a>優化</h2><p>在特徵選擇的前提下保證模型的高效率</p><h3 id="Method-I-優化threshold"><a href="#Method-I-優化threshold" class="headerlink" title="Method I 優化threshold"></a>Method I 優化threshold</h3><ul><li>在上面並未指定<code>threshold</code>參數，預設為<code>None</code><ul><li>刪除了所有L1正則化後 參數(權重)$\theta$($w$)為0 的特徵</li></ul></li><li>對<code>threshold</code>參數進行調整 (繪製<code>threshold</code>學習曲線) 優化模型</li><li>調整<code>threshold</code>值時，就不是使用 L1正則化 選擇特徵，而是<strong>使用模型的屬性<code>coef_</code></strong> 生成的各個係數來選擇<ul><li>係數越大的特徵 對邏輯回歸的影響就越大</li></ul></li></ul><h3 id="Method-II-優化模型本身參數C"><a href="#Method-II-優化模型本身參數C" class="headerlink" title="Method II 優化模型本身參數C"></a>Method II 優化模型本身參數C</h3><ul><li>使用<strong>L1范數</strong>來做特徵選擇，但是調整 邏輯回歸建模時的<code>C</code>這個超參數<ul><li>邏輯回歸建模搭配不同<code>C</code>參數的取值；將不同參數<code>C</code>取值的模型，用來進行嵌入法特徵選擇，並繪製學習曲線</li></ul></li></ul><hr><h1 id="係數累加法"><a href="#係數累加法" class="headerlink" title="係數累加法"></a>係數累加法</h1><hr><h1 id="包裝法"><a href="#包裝法" class="headerlink" title="包裝法"></a>包裝法</h1><ul><li>直接設定需要的特徵個數</li><li>現實運用邏輯回歸時，通常會限定變量個數的需求，包裝法此時會非常方便</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;在進行邏輯回歸模型訓練時，通常需要對數據進行降維&lt;/li&gt;
&lt;li&gt;在高維(&amp;gt;1000個)特徵情況下，可先使用特徵選擇的演算法幫助我們篩選特徵，在根據自己相關的業務常識選擇更為相關的特徵&lt;/li&gt;
&lt;li&gt;在邏輯回歸中，&lt;strong&gt;一般不使用PCA和SVD的降維演算法&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;會抹滅特徵的可解釋性&lt;ul&gt;
&lt;li&gt;降維後會無法解釋特徵與Label的關係&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;在不需探究特徵與標籤之間關係的線性數據上，仍可使用&lt;ul&gt;
&lt;li&gt;著重於對測試樣本進行分類時，可使用&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;在邏輯回歸中，前幾章提到的特徵選擇方法都可以使用
    
    </summary>
    
      <category term="Python機器學習" scheme="http://www.taroballz.com/categories/Python%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92/"/>
    
    
      <category term="Python" scheme="http://www.taroballz.com/tags/Python/"/>
    
      <category term="2019" scheme="http://www.taroballz.com/tags/2019/"/>
    
      <category term="機器學習(Machine Learning)" scheme="http://www.taroballz.com/tags/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-Machine-Learning/"/>
    
      <category term="特徵預處理" scheme="http://www.taroballz.com/tags/%E7%89%B9%E5%BE%B5%E9%A0%90%E8%99%95%E7%90%86/"/>
    
      <category term="特徵工程" scheme="http://www.taroballz.com/tags/%E7%89%B9%E5%BE%B5%E5%B7%A5%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>機器學習-特徵工程-細談特徵降維</title>
    <link href="http://www.taroballz.com/2019/06/16/ML_feature_decomposition/"/>
    <id>http://www.taroballz.com/2019/06/16/ML_feature_decomposition/</id>
    <published>2019-06-15T16:00:00.000Z</published>
    <updated>2019-06-21T17:59:42.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><ul><li>sklearn中的降維演算法都被包括在模塊<code>decomposition</code>中<ul><li><code>decomposition</code>模塊為一個<strong>矩陣分解</strong>模塊</li></ul></li><li>SVD和主成分分析PCA都屬於矩陣分解算法中的入門算法，通過分解特徵矩陣來進行降維</li><li>降維(矩陣分解)的過程中追求<strong>既減少特徵的數量，又保留大量有效信息</strong>的 新特徵矩陣<ul><li>將帶有重複信息(特徵與特徵之間有線性相關)的特徵合併</li><li>刪除無效信息(noise)的特徵</li></ul></li><li>如果一個特徵的方差(Variance)很大，則可以說明這個特徵帶有大量信息</li><li>兩種主要的降維方法；其<strong>矩陣分解的方法</strong>不同，<strong>信息量衡量指標</strong>不同，但都涉及大量矩陣運算<ul><li>主成分分析(PCA)</li><li>奇異值分解(SVD)</li></ul></li><li>通常在能進行PCA降維的情況下，不會進行特徵選擇<ul><li>無法使用PCA降維的情況下才會做特徵選擇<a id="more"></a></li></ul></li></ul><h1 id="主成分分析-PCA"><a href="#主成分分析-PCA" class="headerlink" title="主成分分析(PCA)"></a>主成分分析(PCA)</h1><ul><li>PCA使用的信息量衡量指標，就是樣本方差(Variance)；又稱可解釋性方差，<strong>其值越大，特徵所帶的信息量越多</strong></li><li><p>樣本方差公式如下</p><script type="math/tex; mode=display">Var = \frac{1}{n-1} \sum_{i=1}^{n} (x_i-\hat{x})^2</script><ul><li>$Var$代表 一個特徵的方差</li><li>$n$代表樣本量</li><li>$x_i$代表一個特徵中的每個樣本的取值</li><li>$\hat{x}$代表該列樣本的均值</li></ul></li><li><p>PCA本質就是將<strong>已存在的特徵進行壓縮</strong>，降維後的特徵不是原本特徵中的任何一個特徵，而是<strong>通過某些方式組合起來的新特徵</strong></p><ul><li>新特徵矩陣生成<strong>不具有可讀性</strong></li><li>以PCA為代表的 降維方法 是屬於特徵創造(feature creation)<ul><li>在<strong>線性模型當中，不能使用PCA</strong><ul><li>通常使用<strong>特徵選擇</strong>進行降維</li></ul></li></ul></li></ul></li><li><p>參考<a href="http://www.taroballz.com/2018/07/06/ML_DecreaseFeature/#%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90-PCA">http://www.taroballz.com/2018/07/06/ML_DecreaseFeature/#%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90-PCA</a></p></li></ul><h1 id="奇異值分解-SVD"><a href="#奇異值分解-SVD" class="headerlink" title="奇異值分解(SVD)"></a>奇異值分解(SVD)</h1><ul><li>奇異值分解可以在不計算協方差矩陣等等複雜計算冗長的情況下，<strong>直接求出新特徵空間和降維後的特徵矩陣</strong></li><li>SVD在矩陣分解中會比PCA簡單快速</li><li>SVD的信息衡量指標為<strong>奇異值</strong>  比較複雜</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;sklearn中的降維演算法都被包括在模塊&lt;code&gt;decomposition&lt;/code&gt;中&lt;ul&gt;
&lt;li&gt;&lt;code&gt;decomposition&lt;/code&gt;模塊為一個&lt;strong&gt;矩陣分解&lt;/strong&gt;模塊&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;SVD和主成分分析PCA都屬於矩陣分解算法中的入門算法，通過分解特徵矩陣來進行降維&lt;/li&gt;
&lt;li&gt;降維(矩陣分解)的過程中追求&lt;strong&gt;既減少特徵的數量，又保留大量有效信息&lt;/strong&gt;的 新特徵矩陣&lt;ul&gt;
&lt;li&gt;將帶有重複信息(特徵與特徵之間有線性相關)的特徵合併&lt;/li&gt;
&lt;li&gt;刪除無效信息(noise)的特徵&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;如果一個特徵的方差(Variance)很大，則可以說明這個特徵帶有大量信息&lt;/li&gt;
&lt;li&gt;兩種主要的降維方法；其&lt;strong&gt;矩陣分解的方法&lt;/strong&gt;不同，&lt;strong&gt;信息量衡量指標&lt;/strong&gt;不同，但都涉及大量矩陣運算&lt;ul&gt;
&lt;li&gt;主成分分析(PCA)&lt;/li&gt;
&lt;li&gt;奇異值分解(SVD)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;通常在能進行PCA降維的情況下，不會進行特徵選擇&lt;ul&gt;
&lt;li&gt;無法使用PCA降維的情況下才會做特徵選擇
    
    </summary>
    
      <category term="Python機器學習" scheme="http://www.taroballz.com/categories/Python%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92/"/>
    
    
      <category term="Python" scheme="http://www.taroballz.com/tags/Python/"/>
    
      <category term="2019" scheme="http://www.taroballz.com/tags/2019/"/>
    
      <category term="機器學習(Machine Learning)" scheme="http://www.taroballz.com/tags/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-Machine-Learning/"/>
    
      <category term="特徵工程" scheme="http://www.taroballz.com/tags/%E7%89%B9%E5%BE%B5%E5%B7%A5%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>機器學習-特徵工程-特徵選擇-包裝法</title>
    <link href="http://www.taroballz.com/2019/06/15/ML_feature_selection_wrapper_method/"/>
    <id>http://www.taroballz.com/2019/06/15/ML_feature_selection_wrapper_method/</id>
    <published>2019-06-14T16:00:00.000Z</published>
    <updated>2019-06-21T17:32:46.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><ul><li>包裝法(wrapper)仍是一種特徵選擇和演算法訓練同時進行的方法</li><li>與嵌入法相似的地方，仍是依賴算法有<code>coef_</code>及<code>feature_importances_</code>等屬性來完成特徵選擇</li><li>不同之處為 <strong>使用一個專業的數據挖掘演算法；而非使用 在評估模型使用的演算法</strong></li><li><p><a href="https://i.imgur.com/cc9NMcv.png" target="_blank" rel="noopener">wrapper method</a></p><ul><li>黑箱中所使用的Learning Algorithm是一個目標函數，專門用於選取特徵<ul><li><strong>不需像過濾法那樣在指定評估指標</strong> 和 threshold值</li></ul></li></ul></li><li><p>計算成本位於嵌入法與過濾法中間</p></li><li>包裝法效果是所有特徵選擇方法中最利於提升模型表現的</li><li>不適用於大</li><li>包裝法相比嵌入法更能保證模型效果</li></ul><a id="more"></a><h1 id="Process"><a href="#Process" class="headerlink" title="Process"></a>Process</h1><ol><li>在初始特徵(全部特徵)集上訓練黑箱中的Learning Algorithm</li><li>獲得<code>coef</code>, <code>feature_importances_</code>等屬性作為重要程度指標</li><li>除去最不重要的特徵，得到修剪的特徵集合</li><li>回到步驟1，直到<strong>達到所指定所需數量的特徵</strong></li></ol><h1 id="遞歸特徵消除法-RFE"><a href="#遞歸特徵消除法-RFE" class="headerlink" title="遞歸特徵消除法(RFE)"></a>遞歸特徵消除法(RFE)</h1><ul><li>最典型的Learning Algorithm為遞歸特徵消除法(Recursive feature elimination,RFE)</li><li>為一<strong>貪婪</strong>優化演算法,旨在找尋最佳的特徵子集</li><li><code>from sklearn.feature_selection import RFE</code></li></ul><h2 id="重要參數"><a href="#重要參數" class="headerlink" title="重要參數"></a>重要參數</h2><h3 id="estimator"><a href="#estimator" class="headerlink" title="estimator"></a>estimator</h3><p>填寫實例化後的演算法模型實例</p><h3 id="n-features-to-select"><a href="#n-features-to-select" class="headerlink" title="n_features_to_select"></a>n_features_to_select</h3><p>填寫想要選擇出來的特徵個數</p><ul><li>預設為<code>None</code></li><li>意指想保留的維度為多少</li><li>為一<strong>超參數</strong><ul><li>可使用<strong>學習曲線法</strong>進行優化</li></ul></li></ul><h3 id="step"><a href="#step" class="headerlink" title="step"></a>step</h3><p>填寫每次迭代中欲移除的特徵個數</p><ul><li>預設為<code>1</code></li></ul><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>與其他轉換器(transformer)一樣皆具有<code>fit_transform</code>、<code>fit</code>、<code>transform</code>、<code>inverse_transform</code> 等方法</p><h2 id="重要屬性"><a href="#重要屬性" class="headerlink" title="重要屬性"></a>重要屬性</h2><h3 id="support"><a href="#support" class="headerlink" title=".support_"></a>.support_</h3><p>返回所有的特徵是否最後被選中的boolean矩陣</p><h3 id="ranking"><a href="#ranking" class="headerlink" title=".ranking_"></a>.ranking_</h3><p>返回特徵 按數次迭代中綜合重要性的排名</p><ul><li>排名越前面的特徵越重要</li></ul><h2 id="RFECV"><a href="#RFECV" class="headerlink" title="RFECV"></a>RFECV</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> feature_selection <span class="keyword">import</span> RFECV</span><br></pre></td></tr></table></figure><ul><li>其會在交叉驗證循環中執行<code>RFE</code>以<strong>找到最佳數量</strong>的特徵</li><li>增加參數<code>cv</code>(交叉驗證的次數)，其他用法與<code>RFE</code>一模一樣</li></ul><h1 id="特徵選擇總結"><a href="#特徵選擇總結" class="headerlink" title="特徵選擇總結"></a>特徵選擇總結</h1><ul><li>過濾法較快速但是粗糙</li><li>包裝法和嵌入法更精確，計算量較大，運行時間長</li><li>數據量大時，優先使用方差過濾(<code>VarianceThreshold</code>)和互信息法(mutal_info_classif(regression))，再使用其他的特徵選擇方法</li><li>Logistic Regression：優先使用嵌入法</li><li>SVM：優先使用包裝法</li><li>不知道用哪個時，先使用過濾法</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;包裝法(wrapper)仍是一種特徵選擇和演算法訓練同時進行的方法&lt;/li&gt;
&lt;li&gt;與嵌入法相似的地方，仍是依賴算法有&lt;code&gt;coef_&lt;/code&gt;及&lt;code&gt;feature_importances_&lt;/code&gt;等屬性來完成特徵選擇&lt;/li&gt;
&lt;li&gt;不同之處為 &lt;strong&gt;使用一個專業的數據挖掘演算法；而非使用 在評估模型使用的演算法&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://i.imgur.com/cc9NMcv.png&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;wrapper method&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;黑箱中所使用的Learning Algorithm是一個目標函數，專門用於選取特徵&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;不需像過濾法那樣在指定評估指標&lt;/strong&gt; 和 threshold值&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;計算成本位於嵌入法與過濾法中間&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;包裝法效果是所有特徵選擇方法中最利於提升模型表現的&lt;/li&gt;
&lt;li&gt;不適用於大&lt;/li&gt;
&lt;li&gt;包裝法相比嵌入法更能保證模型效果&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Python機器學習" scheme="http://www.taroballz.com/categories/Python%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92/"/>
    
    
      <category term="Python" scheme="http://www.taroballz.com/tags/Python/"/>
    
      <category term="2019" scheme="http://www.taroballz.com/tags/2019/"/>
    
      <category term="機器學習(Machine Learning)" scheme="http://www.taroballz.com/tags/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-Machine-Learning/"/>
    
      <category term="特徵工程" scheme="http://www.taroballz.com/tags/%E7%89%B9%E5%BE%B5%E5%B7%A5%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>機器學習-特徵工程-特徵選擇-嵌入法</title>
    <link href="http://www.taroballz.com/2019/06/14/ML_feature_selection_embedded_method/"/>
    <id>http://www.taroballz.com/2019/06/14/ML_feature_selection_embedded_method/</id>
    <published>2019-06-13T16:00:00.000Z</published>
    <updated>2019-06-21T17:30:53.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><ul><li>嵌入法(Embedded)是讓演算法自己決定使用那些特徵的方法<ul><li>特徵選擇 及 演算法訓練 <strong>同時進行</strong></li><li>特徵的選擇是依賴模型的表現來進行選擇，因此為一循環過程</li><li><a href="https://i.imgur.com/30KO8Y1.png" target="_blank" rel="noopener">Embedded method</a><ul><li>最後得到各個特徵的權值，根據權值從大到小選擇特徵</li></ul></li></ul></li><li>相比過濾法，嵌入法的篩選的特徵更為精確，對模型更有效</li><li>缺點1：無法有效界定<strong>有用特徵 權值係數的臨界值</strong><ul><li>不像過濾法有p值可幫助我們做界定</li><li>只能說要是權值係數為0時，則對模型毫無貢獻</li><li>改善方法：<ul><li>權值係數作為 <strong>超參數</strong>，使用<strong>學習曲線方法</strong>進行調參</li><li>根據 <strong>模型本身性質</strong> 判斷權值係數範圍</li></ul></li></ul></li><li>缺點2：因為其是<strong>引入演算法</strong>來挑選特徵，且會<strong>使用全部特徵</strong><ul><li>計算時間與所使用的演算法 及 數據量有關係</li></ul></li><li><code>from sklearn.feature_selection import SelectFromModel</code>    </li></ul><a id="more"></a><h1 id="SelectFromModel"><a href="#SelectFromModel" class="headerlink" title="SelectFromModel"></a>SelectFromModel</h1><ul><li>為一種 元轉換器</li><li>可與任何擬合後具有<code>coef_</code>, <code>feature_importances_</code>屬性 或 參數中可選懲罰項的評估器合用<ul><li>RandomForest, DecisionTree：具有<code>feature_importances_</code>(取值範圍為0-1)<ul><li>若重要性低於閥值：特徵不重要</li></ul></li><li>LogisticRegression：l1,l2 懲罰項</li><li>SVM：l2 懲罰項</li></ul></li></ul><h2 id="重要參數"><a href="#重要參數" class="headerlink" title="重要參數"></a>重要參數</h2><h3 id="estimator"><a href="#estimator" class="headerlink" title="estimator"></a>estimator</h3><p>所使用的演算法模型，只要是帶<code>feature_importances_</code> or <code>coef_</code> 屬性 or 帶有 l1, l2懲罰項的演算法都可以使用</p><ul><li><strong>必須將模型實例化後，填入實例化的模型變量</strong></li></ul><h3 id="threshold"><a href="#threshold" class="headerlink" title="threshold"></a>threshold</h3><p>特徵重要性的閥值，重要性低於指定的值都將被刪除</p><ul><li><strong>挑選threshold的值非常重要，其決定刪掉多少特徵</strong></li><li>其為<strong>超參數</strong><ul><li><strong>使用學習曲線方法調教此參數</strong></li></ul></li></ul><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h3 id="fit-transform"><a href="#fit-transform" class="headerlink" title="fit_transform"></a>fit_transform</h3><p>與其他轉換器不一樣的是<code>fit_transform</code>參數中須放入兩個參數</p><ul><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_Embedded = SelectFromModel(estimator=RFC_,threshold=<span class="number">0.005</span>).fit_transform(X,y)</span><br></pre></td></tr></table></figure><ul><li>第一個參數須放入欲轉換的feature矩陣<code>X</code></li><li>第二個參數為Label矩陣<code>y</code></li></ul></li></ul><h1 id="總結"><a href="#總結" class="headerlink" title="總結"></a>總結</h1><ul><li>比起必須思考很多統計量的過濾法來說，嵌入法可能是更有效的選擇</li><li>過濾法計算較嵌入法快<ul><li>大型數據中，優先使用過濾法</li><li>或使用結合過濾法和嵌入法的包裝法(wrapper)</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;嵌入法(Embedded)是讓演算法自己決定使用那些特徵的方法&lt;ul&gt;
&lt;li&gt;特徵選擇 及 演算法訓練 &lt;strong&gt;同時進行&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;特徵的選擇是依賴模型的表現來進行選擇，因此為一循環過程&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://i.imgur.com/30KO8Y1.png&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Embedded method&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;最後得到各個特徵的權值，根據權值從大到小選擇特徵&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;相比過濾法，嵌入法的篩選的特徵更為精確，對模型更有效&lt;/li&gt;
&lt;li&gt;缺點1：無法有效界定&lt;strong&gt;有用特徵 權值係數的臨界值&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;不像過濾法有p值可幫助我們做界定&lt;/li&gt;
&lt;li&gt;只能說要是權值係數為0時，則對模型毫無貢獻&lt;/li&gt;
&lt;li&gt;改善方法：&lt;ul&gt;
&lt;li&gt;權值係數作為 &lt;strong&gt;超參數&lt;/strong&gt;，使用&lt;strong&gt;學習曲線方法&lt;/strong&gt;進行調參&lt;/li&gt;
&lt;li&gt;根據 &lt;strong&gt;模型本身性質&lt;/strong&gt; 判斷權值係數範圍&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;缺點2：因為其是&lt;strong&gt;引入演算法&lt;/strong&gt;來挑選特徵，且會&lt;strong&gt;使用全部特徵&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;計算時間與所使用的演算法 及 數據量有關係&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;from sklearn.feature_selection import SelectFromModel&lt;/code&gt;    &lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Python機器學習" scheme="http://www.taroballz.com/categories/Python%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92/"/>
    
    
      <category term="Python" scheme="http://www.taroballz.com/tags/Python/"/>
    
      <category term="2019" scheme="http://www.taroballz.com/tags/2019/"/>
    
      <category term="機器學習(Machine Learning)" scheme="http://www.taroballz.com/tags/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-Machine-Learning/"/>
    
      <category term="特徵工程" scheme="http://www.taroballz.com/tags/%E7%89%B9%E5%BE%B5%E5%B7%A5%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>機器學習-特徵工程-特徵選擇(feature_selection)-過濾法</title>
    <link href="http://www.taroballz.com/2019/06/12/ML_feature_selection_filter_method/"/>
    <id>http://www.taroballz.com/2019/06/12/ML_feature_selection_filter_method/</id>
    <published>2019-06-11T16:00:00.000Z</published>
    <updated>2019-07-08T16:44:55.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>從所有的特徵中，選擇出<strong>有意義的</strong>或<strong>對模型有幫助的特徵</strong></p><ul><li><strong>避免必須將所有特徵都導入模型進行訓練的窘境</strong></li><li>重要！！：必須與數據提供者討論</li><li>若是我們無法找到合適領域的人幫助理解數據來選擇特徵的話，可使用以下四種方法<ul><li>過濾法</li><li>嵌入法</li><li>包裝法</li><li>降維演算法</li></ul></li></ul><a id="more"></a><h1 id="過濾法-Filter"><a href="#過濾法-Filter" class="headerlink" title="過濾法(Filter)"></a>過濾法(Filter)</h1><ul><li>常被用作特徵預處理的步驟</li><li>根據各種<strong>統計檢驗的分數</strong> 及 <strong>相關性的各項指標</strong>來進行特徵選擇</li></ul><h1 id="方差過濾法-VarianceThreshold"><a href="#方差過濾法-VarianceThreshold" class="headerlink" title="方差過濾法(VarianceThreshold)"></a>方差過濾法(VarianceThreshold)</h1><p>請參考<a href="http://www.taroballz.com/2018/07/06/ML_DecreaseFeature/">http://www.taroballz.com/2018/07/06/ML_DecreaseFeature/</a></p><h1 id="相關性過濾"><a href="#相關性過濾" class="headerlink" title="相關性過濾"></a>相關性過濾</h1><p>方差過濾完畢後，我們希望挑選出與標籤(Label)相關且有意義的特徵</p><ul><li>若特徵與標籤無關：浪費計算資源</li><li>有三種常用的方法來評判特徵與標籤之間的相關性<ul><li>卡方過濾</li><li>F-score檢驗</li><li>互信息</li></ul></li></ul><h2 id="卡方過濾"><a href="#卡方過濾" class="headerlink" title="卡方過濾"></a>卡方過濾</h2><ul><li>專門針對<strong>離散型標籤(分類問題)</strong>的相關性過濾</li><li><code>sklearn.feature_selection.chi2</code><ul><li>計算每個 <strong>非負數特徵</strong> 和 標籤 之間的卡方統計量<ul><li>卡方檢驗法<strong>不能計算負數</strong>；可是用以下兩種預處理方法變為正數<ul><li><code>MinMaxScalar</code></li><li><code>StandardScalar</code></li></ul></li></ul></li><li>依照卡方統計量 由高到低 為 特徵排名</li></ul></li><li>再結合<code>feature_selection.SelectKBest</code><ul><li>輸入 <strong>評分標準</strong> 來選出前<code>K</code>個分數最高的特徵<ul><li>除去最可能獨立於標籤，與分類目的無關的特徵</li></ul></li></ul></li><li>若卡方檢驗檢測到某個特徵所有值都相同時，會提示先進行方差過濾</li></ul><h3 id="SelectKBest"><a href="#SelectKBest" class="headerlink" title="SelectKBest()"></a>SelectKBest()</h3><p>為一轉換器，有兩個重要的參數<code>score_func</code> 及 <code>k</code></p><ul><li><code>score_func</code><ul><li>指定轉換器<strong>用於評估的統計量</strong>，可以是<code>chi2</code>等標準</li><li>ref：<a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html" target="_blank" rel="noopener">https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html</a></li></ul></li><li><code>k</code><ul><li>選出前<code>k</code>個 卡方值最高 的特徵</li><li>可指定自己所需要的特徵數目</li><li>為一<strong>超參數</strong>，設定過低的值有可能會刪除與模型相關且有效的特徵，須不斷進行調整<ul><li>使用<strong>學習曲線驗證法</strong>得出好的<code>k</code>值<ul><li>計算成本龐大</li></ul></li><li>實例化<code>chi2</code>等評估指標 獲得<strong>卡方值很大</strong>且<strong>P值&lt;0.05</strong> 的特徵數作為<code>k</code>值<ul><li>請參考 <strong>卡方檢驗</strong> 章</li></ul></li></ul></li></ul></li></ul><h4 id="fit-transform"><a href="#fit-transform" class="headerlink" title="fit_transform"></a>fit_transform</h4><p>與其他轉換器不一樣的是<code>fit_transform</code>參數中須放入兩個參數</p><ul><li><code>X_fschi = SelectKBest(chi2,k=300).fit_transform(X,y)</code><ul><li>第一個參數須放入欲轉換的feature矩陣<code>X</code></li><li>第二個參數為Label矩陣<code>y</code></li></ul></li></ul><h3 id="卡方檢驗"><a href="#卡方檢驗" class="headerlink" title="卡方檢驗"></a>卡方檢驗</h3><ul><li>推測兩組數據(feature,Label)之間的差異<ul><li>原假設：<strong>兩組數據是互相獨立</strong>的</li></ul></li><li><p>檢驗後返回 <strong>卡方值</strong> 及 <strong>P值</strong> 兩個統計量</p><ul><li>卡方值：難界定有效篩選的範圍</li><li>P值：一般使用 0.01 or 0.05 作為<strong>顯著性水平</strong>(p值判斷邊界)<ul><li>>=0.05 or 0.01：兩組數據相關 (拒絕原假設，接受備擇假設)<ul><li>該特徵對模型訓練有貢獻</li></ul></li><li>\&lt;0.05 or 0.01：兩組數據相互獨立 (接受原假設)</li></ul></li></ul></li><li><p>因此須選取 <strong>卡方值很大</strong>且<strong>P值>=0.05 or 0.01</strong> 的特徵(與標籤相關的特徵)</p><ul><li><p>可從<code>chi2</code>實例獲得 <strong>各個特徵對應的 卡方值 及 P值</strong></p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> chi2</span><br><span class="line"></span><br><span class="line">chivalue,pvalues_chi = chi2(X,y)</span><br></pre></td></tr></table></figure><ul><li><code>X</code>為feature矩陣</li><li><code>y</code>為Label矩陣</li><li><code>chivalue</code>為各特徵的卡方值，為一ndarray</li><li><code>pvalues</code>為各特徵相對標籤的p值，為一ndarray</li></ul></li><li><code>K_value = chi_values.shape[0] - (p_values_chi &lt; 0.01).sum()</code> 將得到超參數<code>k</code>值該定為多少</li></ul></li></ul><h2 id="F檢驗-ANOVA"><a href="#F檢驗-ANOVA" class="headerlink" title="F檢驗(ANOVA)"></a>F檢驗(ANOVA)</h2><ul><li>又稱作方差齊性檢驗(變異數分析;Analysis of variance)</li><li>是用來捕捉每個<strong>特徵與標籤之間的 <em>線性</em> 關係</strong>的過濾方法<ul><li>原假設：<strong>數據不存在顯著的線性關係</strong></li></ul></li><li>F檢驗分類<ul><li><code>feature_selection.f_classif</code></li><li>用於Label為離散型數據</li></ul></li><li>F檢驗回歸<ul><li><code>feature_selection.f_regression</code></li><li>用於Label為連續型數據</li></ul></li><li><p>檢驗後返回 <strong>F值</strong> 及 <strong>P值</strong> 兩個統計量</p><ul><li>F值：難界定有效篩選的範圍</li><li>P值：一般使用 0.01 or 0.05 作為<strong>顯著性水平</strong>(p值判斷邊界)<ul><li>&lt;=0.05 or 0.01：兩組數據存在 顯著線性相關 (拒絕原假設，接受備擇假設)<ul><li>該特徵對模型訓練有貢獻</li></ul></li><li>>0.05 or 0.01：兩組數據 沒有顯著線性關係 (接受原假設)<ul><li>應被刪除</li></ul></li></ul></li></ul></li><li><p>選取<strong>P值&lt;0.05 or 0.01</strong> 的特徵(與標籤相關的特徵)</p><ul><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> f_classif</span><br><span class="line"></span><br><span class="line">F, pvalues_f = f_classif(X,y)</span><br></pre></td></tr></table></figure><p>*<code>pvalues</code>為各特徵相對標籤的p值，為一ndarray</p></li><li><code>K_value = F.shape[0] - (pvalues_f &gt; 0.01).sum()</code> 將得到超參數<code>k</code>值該定為多少</li></ul></li><li><p>F檢驗在服從<strong>正態分布</strong>時效果會非常穩定</p><ul><li>如要使用F檢驗過濾特徵，會先將數據轉換為 服從正態分布 的形式<ul><li><code>StandardScalar</code></li></ul></li></ul></li><li><p>再結合<code>feature_selection.SelectKBest</code>一起聯用</p><ul><li>輸入 <strong>評分標準</strong> 來選出前<code>K</code>個分數最高的特徵</li></ul></li></ul><h2 id="互信息法"><a href="#互信息法" class="headerlink" title="互信息法"></a>互信息法</h2><ul><li>是用來捕捉特徵與標籤之間的<strong>任意關係</strong>(線性及非線性)的過濾方法<ul><li>故比F檢驗更強大</li></ul></li><li>互信息分類<ul><li><code>feature_selection.mutual_info_classif</code></li></ul></li><li>互信息回歸<ul><li><code>feature_selection.mutual_info_regression</code></li></ul></li><li><p>返回 <strong>每個特徵 與 標籤 之間的 互信息量的估計</strong></p><ul><li>介於0-1之間<ul><li>0：表示兩組數據相互獨立</li><li>1：表示兩組數據<strong>完全相關</strong></li></ul></li></ul></li><li><p>選取 <strong>結果值大於0</strong>的特徵(與標籤相關的特徵)</p><ul><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> mutual_info_classif <span class="keyword">as</span> MIC</span><br><span class="line"></span><br><span class="line">result = MIC(X,y)</span><br></pre></td></tr></table></figure><ul><li><code>result</code>為各特徵的戶信息量估計  </li></ul></li><li>只要不等於0就是與標籤有關<ul><li><code>K_values_all_MIC = result.shape[0] -(result == 0).sum()</code> 可得到超參數<code>k</code>值該定為多少</li></ul></li></ul></li></ul><h1 id="總結"><a href="#總結" class="headerlink" title="總結"></a>總結</h1><p>先使用方差過濾，在使用各種過濾法來捕捉相關性，可直接使用互信息法更好</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h1&gt;&lt;p&gt;從所有的特徵中，選擇出&lt;strong&gt;有意義的&lt;/strong&gt;或&lt;strong&gt;對模型有幫助的特徵&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;避免必須將所有特徵都導入模型進行訓練的窘境&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;重要！！：必須與數據提供者討論&lt;/li&gt;
&lt;li&gt;若是我們無法找到合適領域的人幫助理解數據來選擇特徵的話，可使用以下四種方法&lt;ul&gt;
&lt;li&gt;過濾法&lt;/li&gt;
&lt;li&gt;嵌入法&lt;/li&gt;
&lt;li&gt;包裝法&lt;/li&gt;
&lt;li&gt;降維演算法&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Python機器學習" scheme="http://www.taroballz.com/categories/Python%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92/"/>
    
    
      <category term="Python" scheme="http://www.taroballz.com/tags/Python/"/>
    
      <category term="2019" scheme="http://www.taroballz.com/tags/2019/"/>
    
      <category term="機器學習(Machine Learning)" scheme="http://www.taroballz.com/tags/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-Machine-Learning/"/>
    
      <category term="特徵工程" scheme="http://www.taroballz.com/tags/%E7%89%B9%E5%BE%B5%E5%B7%A5%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>機器學習-特徵預處理-處理連續型數據</title>
    <link href="http://www.taroballz.com/2019/06/09/ML_continueVar_Preprocessing/"/>
    <id>http://www.taroballz.com/2019/06/09/ML_continueVar_Preprocessing/</id>
    <published>2019-06-08T16:23:54.000Z</published>
    <updated>2019-06-11T17:18:55.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><ul><li><p>二值化(binarizer)</p><ul><li>根據閥值(threshold)將數值二元化(大於閥值設為1;小於or等於閥值設為0)<ul><li>用於處理連續型變量<ul><li>連續型變量劃分為二分類</li><li>決定僅考慮某種現象存在與否</li></ul></li></ul></li></ul></li><li><p>分段(KBinsDiscretizer;分箱)</p><ul><li>將連續型數據劃分為不同分類變量<ul><li>例如：根據不同的年齡段分成老年、中年、青少年、小孩、幼齡</li></ul></li></ul></li></ul><a id="more"></a><h1 id="二值化-Binarizer"><a href="#二值化-Binarizer" class="headerlink" title="二值化(Binarizer)"></a>二值化(<code>Binarizer</code>)</h1><ul><li><code>from sklearn.preprocessing import Binarizer</code></li><li><code>Binarizer</code>為特徵專用，不能使用一維數據，需進行<code>reshape(-1,1)</code>升維成二維 一列的數據</li><li>實例化指定<code>threshold</code><ul><li><code>Binarizer(threshold=?)</code></li><li>大於<code>threshold</code>值會設為1，小於等於<code>threshold</code>值會設為0</li></ul></li></ul><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>與其他轉換器(transformer)一樣皆具有<code>fit_transform</code>、<code>fit</code>、<code>transform</code>、<code>inverse_transform</code> 等方法</p><h1 id="分段-KBinsDiscretizer"><a href="#分段-KBinsDiscretizer" class="headerlink" title="分段(KBinsDiscretizer)"></a>分段(<code>KBinsDiscretizer</code>)</h1><ul><li><code>from sklearn.preprocessing import KBinsDiscretizer</code></li><li>將連續型數據排序後，按順序分箱後編碼</li><li><code>KBinsDiscretizer</code>為特徵專用，不能使用一維數據，需進行<code>reshape(-1,1)</code>升維成二維 一列的數據</li></ul><h2 id="實例化的重要參數"><a href="#實例化的重要參數" class="headerlink" title="實例化的重要參數"></a>實例化的重要參數</h2><h3 id="n-bins"><a href="#n-bins" class="headerlink" title="n_bins"></a>n_bins</h3><p>每個特徵中分箱的個數</p><ul><li>預設為5，將連續型數據分為5類</li><li>一次會被運用到所有<code>fit</code>進來的特徵矩陣<ul><li>若一次導入有10個feature的特徵矩陣，全部特徵都會被分為指定的類數別</li><li>若希望不同特徵的分箱數不同，則<strong>必須實例化多個不同的分箱依據的<code>KBinsDiscretizer</code>轉換器</strong></li></ul></li></ul><h3 id="encode"><a href="#encode" class="headerlink" title="encode"></a>encode</h3><p>編碼的方式</p><ul><li>預設為<code>&quot;onehot&quot;</code>：返回sparse matrix，含有該類別的樣本為1，反之為0<ul><li>分為多少箱就有多少列</li></ul></li><li><code>&quot;ordinal&quot;</code>：每個特徵(箱)被編碼為一個整數，返回一列含有不同整數編碼的箱的矩陣</li><li><code>&quot;onehot-dense&quot;</code>：做onehot編碼後，返回一個密集數組</li></ul><h3 id="strategy"><a href="#strategy" class="headerlink" title="strategy"></a>strategy</h3><p>用來定義箱寬的方式</p><ul><li>預設為<code>&quot;quantile&quot;</code>：表示等位分箱，<strong>每個特徵中的每個箱內的樣本數量相同</strong><ul><li>根據<strong>箱內樣本的數量</strong>進行分割</li></ul></li><li><code>&quot;uniform&quot;</code>：表示等寬分箱，每個特徵中的每個箱最大值之間差為(<code>(特徵.max() - 特徵.min()) / (n_bins)</code>)<ul><li>根據數據的<strong>值</strong>來進行分箱</li></ul></li><li><code>&quot;kmeans&quot;</code>：表示按聚類分箱<ul><li>每個箱中的值到最近的一維k均值聚類的簇心的距離相同</li></ul></li></ul><h2 id="方法-1"><a href="#方法-1" class="headerlink" title="方法"></a>方法</h2><p>與其他轉換器(transformer)一樣皆具有<code>fit_transform</code>、<code>fit</code>、<code>transform</code>、<code>inverse_transform</code> 等方法</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="http://www.peixun.net/view/1281.html" target="_blank" rel="noopener">http://www.peixun.net/view/1281.html</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;二值化(binarizer)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;根據閥值(threshold)將數值二元化(大於閥值設為1;小於or等於閥值設為0)&lt;ul&gt;
&lt;li&gt;用於處理連續型變量&lt;ul&gt;
&lt;li&gt;連續型變量劃分為二分類&lt;/li&gt;
&lt;li&gt;決定僅考慮某種現象存在與否&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;分段(KBinsDiscretizer;分箱)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;將連續型數據劃分為不同分類變量&lt;ul&gt;
&lt;li&gt;例如：根據不同的年齡段分成老年、中年、青少年、小孩、幼齡&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Python機器學習" scheme="http://www.taroballz.com/categories/Python%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92/"/>
    
    
      <category term="Python" scheme="http://www.taroballz.com/tags/Python/"/>
    
      <category term="2019" scheme="http://www.taroballz.com/tags/2019/"/>
    
      <category term="機器學習(Machine Learning)" scheme="http://www.taroballz.com/tags/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-Machine-Learning/"/>
    
      <category term="特徵預處理" scheme="http://www.taroballz.com/tags/%E7%89%B9%E5%BE%B5%E9%A0%90%E8%99%95%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>機器學習-特徵預處理(處理分類型數據)-編碼(Encode)</title>
    <link href="http://www.taroballz.com/2019/06/05/ML_Encoder/"/>
    <id>http://www.taroballz.com/2019/06/05/ML_Encoder/</id>
    <published>2019-06-04T16:08:00.000Z</published>
    <updated>2019-06-11T17:18:55.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><ul><li>將文字型的數據轉換為<strong>數值型</strong>數據</li></ul><a id="more"></a><h1 id="分類標籤轉換為分類數值-LabelEncoer"><a href="#分類標籤轉換為分類數值-LabelEncoer" class="headerlink" title="分類標籤轉換為分類數值(LabelEncoer)"></a>分類標籤轉換為分類數值(<code>LabelEncoer</code>)</h1><ul><li>為標籤(Label)專用的，能夠將文字分類轉換為分類數值</li><li>位於<code>sklearn.preprocessing</code>中</li><li>實例化成物件後，因其接受的數據為標籤，所以允許一維數據的輸入<ul><li>不用將其轉換為二維  一列的數據</li></ul></li></ul><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>與其他轉換器(transformer)一樣皆具有<code>fit_transform</code>、<code>fit</code>、<code>transform</code>、<code>inverse_transform</code> 等方法</p><h2 id="屬性"><a href="#屬性" class="headerlink" title="屬性"></a>屬性</h2><h3 id="classes"><a href="#classes" class="headerlink" title=".classes_"></a>.classes_</h3><ul><li>查看標籤中有哪些類別</li><li><code>LabelEncoder.classes_</code></li></ul><h1 id="分類特徵轉換為分類數值-OrdinalEncoder"><a href="#分類特徵轉換為分類數值-OrdinalEncoder" class="headerlink" title="分類特徵轉換為分類數值(OrdinalEncoder)"></a>分類特徵轉換為分類數值(<code>OrdinalEncoder</code>)</h1><ul><li>為特徵專用的，將文字分類特徵轉換為數值</li><li>位於<code>sklearn.preprocessing</code>中</li><li>不允許導入一維的數據</li></ul><h2 id="方法-1"><a href="#方法-1" class="headerlink" title="方法"></a>方法</h2><p>與其他轉換器(transformer)一樣皆具有<code>fit_transform</code>、<code>fit</code>、<code>transform</code>、<code>inverse_transform</code> 等方法</p><h2 id="屬性-1"><a href="#屬性-1" class="headerlink" title="屬性"></a>屬性</h2><h3 id="categories"><a href="#categories" class="headerlink" title=".categories_"></a>.categories_</h3><ul><li>查看各個特徵中有哪些類別</li><li><code>OrdinalEncoder.categories_</code></li></ul><h1 id="one-hot編碼-OneHotEncoder"><a href="#one-hot編碼-OneHotEncoder" class="headerlink" title="one-hot編碼(OneHotEncoder)"></a>one-hot編碼(<code>OneHotEncoder</code>)</h1><ul><li>用作表示互相獨立且不可計算的變量，例如性別，進船的艙門等</li><li>位於<code>sklearn.preprocessing</code>中</li><li><code>OneHotEncoder(categories=&quot;auto&quot;)</code><ul><li><code>categories</code>：0.20版本可以直接輸入<code>&quot;auto&quot;</code>會自動遍歷特徵有多少類；0.19版本須用list明確指定各個特徵含有多少類別</li></ul></li></ul><h2 id="方法-2"><a href="#方法-2" class="headerlink" title="方法"></a>方法</h2><p>與其他轉換器(transformer)一樣皆具有<code>fit_transform</code>、<code>fit</code>、<code>transform</code>、<code>inverse_transform</code> 等方法</p><h3 id="transform-與-fit-transform"><a href="#transform-與-fit-transform" class="headerlink" title="transform 與 fit_transform"></a>transform 與 fit_transform</h3><ul><li>其會將載入的數據進行one-hot編碼後輸出，輸出的為sparse matrix</li><li>如欲輸出像前面兩種編碼器的ndarray型式的話，需進行<code>toarray()</code><ul><li><code>data_array = OneHotEncoder.fit_transform(data).toarray()</code></li></ul></li></ul><h3 id="inverse-transform"><a href="#inverse-transform" class="headerlink" title="inverse_transform"></a>inverse_transform</h3><ul><li>依然可以進行還原</li></ul><h3 id="get-feature-names"><a href="#get-feature-names" class="headerlink" title="get_feature_names"></a>get_feature_names</h3><ul><li>查看onehot編碼後的 sparse matrix 各列(column)對應的原特徵的類別</li><li><code>OneHotEncoder.get_feature_names()</code></li></ul><h2 id="tips"><a href="#tips" class="headerlink" title="tips"></a>tips</h2><ul><li>可對標籤Label進行one-hot編碼<ul><li>使用<code>sklearn.preprocessing.LabelBinarizer</code></li><li>在實際處理時並不常見</li></ul></li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="http://www.peixun.net/view/1281.html" target="_blank" rel="noopener">http://www.peixun.net/view/1281.html</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;將文字型的數據轉換為&lt;strong&gt;數值型&lt;/strong&gt;數據&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Python機器學習" scheme="http://www.taroballz.com/categories/Python%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92/"/>
    
    
      <category term="Python" scheme="http://www.taroballz.com/tags/Python/"/>
    
      <category term="2019" scheme="http://www.taroballz.com/tags/2019/"/>
    
      <category term="機器學習(Machine Learning)" scheme="http://www.taroballz.com/tags/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-Machine-Learning/"/>
    
      <category term="特徵預處理" scheme="http://www.taroballz.com/tags/%E7%89%B9%E5%BE%B5%E9%A0%90%E8%99%95%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>機器學習-Data Mining Introduction</title>
    <link href="http://www.taroballz.com/2019/06/03/ML_FeatureEnginering_and_data_preprocessing/"/>
    <id>http://www.taroballz.com/2019/06/03/ML_FeatureEnginering_and_data_preprocessing/</id>
    <published>2019-06-03T13:11:00.000Z</published>
    <updated>2019-06-04T16:00:22.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><ul><li>數據不給力，再高級的算法都沒有用</li></ul><a id="more"></a><hr><h1 id="Data-Mining-五大流程"><a href="#Data-Mining-五大流程" class="headerlink" title="Data Mining 五大流程"></a>Data Mining 五大流程</h1><h2 id="1-獲取數據"><a href="#1-獲取數據" class="headerlink" title="1. 獲取數據"></a>1. 獲取數據</h2><h2 id="2-數據預處理"><a href="#2-數據預處理" class="headerlink" title="2. 數據預處理"></a>2. 數據預處理</h2><p>從數據中檢測、糾正或刪除損壞、不正確或不適用的記錄之過程。</p><h3 id="面臨問題"><a href="#面臨問題" class="headerlink" title="面臨問題"></a>面臨問題</h3><ul><li>數據類型不同，有的是文字有的是數字，有的為時間序列</li><li>有的是連續型變量，亦有可能是離散型變量</li><li>數據質量不好，有噪聲、有異常值、有缺失、有錯誤</li><li>數據量太大或太小</li></ul><h3 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h3><p>讓數據適應模型，匹配模型需求</p><h2 id="3-特徵工程"><a href="#3-特徵工程" class="headerlink" title="3. 特徵工程"></a>3. 特徵工程</h2><p>將原始數據轉換為更能代表預測模型的潛在問題特徵的過程</p><ul><li>原始數據不一定能引導模型算出最好的結果</li><li>通過挑選相關特徵、組合特徵、提取有效特徵及創造特徵等手法來實現<ul><li>創造特徵通常以<strong>降維演算法</strong>的方式實現</li></ul></li></ul><h3 id="面臨問題-1"><a href="#面臨問題-1" class="headerlink" title="面臨問題"></a>面臨問題</h3><ul><li>特徵之間有相關性</li><li>特徵和標籤無關</li><li>特徵太多或太小</li><li>特徵無法表現出應有的數據現象、無法展示數據的真實面貌</li></ul><h3 id="目的-1"><a href="#目的-1" class="headerlink" title="目的"></a>目的</h3><ol><li>降低計算成本</li><li>提升模型上限<ul><li>至少保證模型在一個比較好的水平<ul><li>降噪：剔除有不良影響的特徵</li></ul></li></ul></li></ol><h2 id="4-建模"><a href="#4-建模" class="headerlink" title="4. 建模"></a>4. 建模</h2><p>測試模型並預測結果</p><h2 id="5-模型上線"><a href="#5-模型上線" class="headerlink" title="5. 模型上線"></a>5. 模型上線</h2><p>在真實狀況中去驗證模型效果</p><hr>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;數據不給力，再高級的算法都沒有用&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Python機器學習" scheme="http://www.taroballz.com/categories/Python%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92/"/>
    
    
      <category term="Python" scheme="http://www.taroballz.com/tags/Python/"/>
    
      <category term="2019" scheme="http://www.taroballz.com/tags/2019/"/>
    
      <category term="機器學習(Machine Learning)" scheme="http://www.taroballz.com/tags/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-Machine-Learning/"/>
    
      <category term="特徵預處理" scheme="http://www.taroballz.com/tags/%E7%89%B9%E5%BE%B5%E9%A0%90%E8%99%95%E7%90%86/"/>
    
      <category term="特徵工程" scheme="http://www.taroballz.com/tags/%E7%89%B9%E5%BE%B5%E5%B7%A5%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>機器學習-調參</title>
    <link href="http://www.taroballz.com/2019/05/29/ML_modifyparm/"/>
    <id>http://www.taroballz.com/2019/05/29/ML_modifyparm/</id>
    <published>2019-05-28T16:12:10.000Z</published>
    <updated>2019-05-28T17:11:33.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="泛化誤差-Genelization-error"><a href="#泛化誤差-Genelization-error" class="headerlink" title="泛化誤差(Genelization error)"></a>泛化誤差(Genelization error)</h1><p>衡量模型在未知數據上的準確率</p><ul><li>模型在未知數據(測試集or袋外數據)上表現不好時，通常會說模型<strong>泛化程度不夠</strong>或是<strong>泛化誤差</strong>大</li><li>泛化誤差受到模型結構(複雜度)影響</li></ul><a id="more"></a><p><img src="https://i.imgur.com/0KmwRh0.png?1" alt="Genelization error"></p><ul><li>模型太簡單：模型欠擬合，泛化誤差大</li><li>模型太複雜：模型過擬合，泛化能力不夠，泛化誤差大</li><li><strong>只有模型複雜度剛剛好才能夠達到泛化誤差最小</strong></li><li>調參之前要先判斷，模型當前處於圖像的哪一邊</li></ul><h1 id="樹模型調參"><a href="#樹模型調參" class="headerlink" title="樹模型調參"></a>樹模型調參</h1><ul><li>模型太複雜或太簡單，都會讓泛化誤差高，我們追求是位於中間平衡點</li><li>模型太複雜就會過擬合，太簡單就會欠擬合</li><li>對樹模型和樹集成模型來說，樹深度越深，枝葉越多，模型越複雜<ul><li>剪枝就是為了降低模型複雜度</li></ul></li><li>樹模型和樹集成模型的目標，都是減少模型複雜度，將摸型往圖像左邊移動</li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="http://www.peixun.net/view/1281.html" target="_blank" rel="noopener">http://www.peixun.net/view/1281.html</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;泛化誤差-Genelization-error&quot;&gt;&lt;a href=&quot;#泛化誤差-Genelization-error&quot; class=&quot;headerlink&quot; title=&quot;泛化誤差(Genelization error)&quot;&gt;&lt;/a&gt;泛化誤差(Genelization error)&lt;/h1&gt;&lt;p&gt;衡量模型在未知數據上的準確率&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;模型在未知數據(測試集or袋外數據)上表現不好時，通常會說模型&lt;strong&gt;泛化程度不夠&lt;/strong&gt;或是&lt;strong&gt;泛化誤差&lt;/strong&gt;大&lt;/li&gt;
&lt;li&gt;泛化誤差受到模型結構(複雜度)影響&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Python機器學習" scheme="http://www.taroballz.com/categories/Python%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92/"/>
    
    
      <category term="Python" scheme="http://www.taroballz.com/tags/Python/"/>
    
      <category term="2019" scheme="http://www.taroballz.com/tags/2019/"/>
    
      <category term="機器學習(Machine Learning)" scheme="http://www.taroballz.com/tags/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>機器學習-特徵工程-填補缺失值</title>
    <link href="http://www.taroballz.com/2019/05/28/ML_FillNanValue/"/>
    <id>http://www.taroballz.com/2019/05/28/ML_FillNanValue/</id>
    <published>2019-05-27T16:14:00.000Z</published>
    <updated>2019-06-04T16:00:22.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><ul><li>現實中所蒐集的數據不可能完美，往往伴隨著缺失值的存在<br>處理方法通常為：</li></ul><ol><li>直接將含有缺失值的樣本刪除</li><li>直接將含有過多缺失值的特徵列刪除</li><li>使用<code>sklearn.impute.SimpleImputer</code>將均值、中位數、眾數、常數填補數據<ul><li>專門用作填補缺失值的類</li></ul></li><li><strong>使用隨機森林回歸填補缺失值</strong></li></ol><a id="more"></a><h1 id="使用特定值填補缺失值"><a href="#使用特定值填補缺失值" class="headerlink" title="使用特定值填補缺失值"></a>使用特定值填補缺失值</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.impute <span class="keyword">import</span> SimpleImputer</span><br><span class="line">imp_mean = SimpleImputer(missing_values = np.nan, strategy=<span class="string">"mean"</span>,copy=<span class="literal">True</span>)</span><br><span class="line">x_missing_mean = imp_mean.fit_transform(X_missing)</span><br></pre></td></tr></table></figure><ul><li><code>X_missing</code>:是含有缺失值的<code>pandas.dataframe</code>或是<code>numpy.ndarray</code>類型的數據<ul><li><code>x_missing_mean</code>：是使用<strong>均值</strong>填補的結果<ul><li>返回的類型為<code>numpy.ndarray</code>類型</li></ul></li></ul></li><li><code>strategy</code>參數：可為<code>&quot;mean&quot;</code>(平均數[預設])、<code>&quot;median&quot;</code>(中位數)、<code>&quot;most_frequent&quot;</code>(眾數[可用於填充文字])、<code>&quot;constant&quot;</code>(常數[可用於填充文字])<ul><li>若使用<code>&quot;constant&quot;</code>(常數)進行填補，則需再指定<code>fill_value</code>參數為何值<ul><li><code>fill_value = 0</code>：使用0進行填補</li></ul></li></ul></li><li><code>copy</code>: 預設為<code>True</code>:創建特徵矩陣的副本 ; 設為<code>False</code>時，會將缺失值直接填補到原本的特徵矩陣中去</li></ul><h2 id="判斷是否填補完成"><a href="#判斷是否填補完成" class="headerlink" title="判斷是否填補完成"></a>判斷是否填補完成</h2><ol><li>使用<code>pandas.DataFrame.isnull()</code>判斷是否填補完成</li><li>加總每一列的結果確定皆為0，則代表填補完成</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pd.DataFrame(x_missing_mean).isnull().sum()</span><br></pre></td></tr></table></figure><h1 id="使用隨機森林回歸進行填補"><a href="#使用隨機森林回歸進行填補" class="headerlink" title="使用隨機森林回歸進行填補"></a>使用隨機森林回歸進行填補</h1><ul><li>建立在回歸演算法認為特徵和標籤之間存在著某種關係<ul><li>利用標籤 + 其他特徵反推缺失值</li></ul></li><li>此種做法對於某一個特徵大量缺失，其他特徵卻很完整的情況非常適用</li></ul><h2 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h2><ol><li>先遍歷所有特徵，查看有缺失值的為哪些特徵，並從<strong>缺失值最少的特徵開始進行填補</strong><ul><li>填補缺失值最少的特徵所需要的準確信息最少</li></ul></li><li>填補其中一個特徵時，先將其他特徵缺失值用0代替</li><li>劃分測試集及訓練集</li><li>建模後預測該特徵的缺失值</li><li>將預測值放到原本的特徵矩陣中；重複第2步驟</li></ol><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="http://www.peixun.net/view/1281.html" target="_blank" rel="noopener">http://www.peixun.net/view/1281.html</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;現實中所蒐集的數據不可能完美，往往伴隨著缺失值的存在&lt;br&gt;處理方法通常為：&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;直接將含有缺失值的樣本刪除&lt;/li&gt;
&lt;li&gt;直接將含有過多缺失值的特徵列刪除&lt;/li&gt;
&lt;li&gt;使用&lt;code&gt;sklearn.impute.SimpleImputer&lt;/code&gt;將均值、中位數、眾數、常數填補數據&lt;ul&gt;
&lt;li&gt;專門用作填補缺失值的類&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;使用隨機森林回歸填補缺失值&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="Python機器學習" scheme="http://www.taroballz.com/categories/Python%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92/"/>
    
    
      <category term="Python" scheme="http://www.taroballz.com/tags/Python/"/>
    
      <category term="2019" scheme="http://www.taroballz.com/tags/2019/"/>
    
      <category term="機器學習(Machine Learning)" scheme="http://www.taroballz.com/tags/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>機器學習-演算法-隨機森林回歸(RandomForestRegressor)</title>
    <link href="http://www.taroballz.com/2019/05/26/ML_RandomForest_Regressor/"/>
    <id>http://www.taroballz.com/2019/05/26/ML_RandomForest_Regressor/</id>
    <published>2019-05-25T16:00:00.000Z</published>
    <updated>2019-05-26T08:12:13.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="DecisionTreeClassifier"><a href="#DecisionTreeClassifier" class="headerlink" title="DecisionTreeClassifier"></a>DecisionTreeClassifier</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">class sklearn.ensemble.RandomForestRegressor(n_estimators=’warn’, criterion=’mse’, </span><br><span class="line">max_depth=None, min_samples_split=2, min_samples_leaf=1, </span><br><span class="line">min_weight_fraction_leaf=0.0, max_features=’auto’, max_leaf_nodes=None, </span><br><span class="line">min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, </span><br><span class="line">oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False)</span><br></pre></td></tr></table></figure><a id="more"></a><ul><li>其大部分的參數、屬性、接口全部和隨機森林回歸(<code>RandomForestClassifier</code>)<ul><li>僅只有回歸樹和分類樹的不同</li><li>不純度指標，參數<code>criterion</code>不一致</li></ul></li></ul><h1 id="重要參數"><a href="#重要參數" class="headerlink" title="重要參數"></a>重要參數</h1><ul><li><code>criterion</code>:參閱回歸樹介紹</li></ul><h1 id="tips"><a href="#tips" class="headerlink" title="tips"></a>tips</h1><ul><li>因其不存在每個樣本被分類到某個標籤的概率問題，所以並沒有<code>predict_proba</code>的接口可以使用</li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="http://www.peixun.net/view/1281.html" target="_blank" rel="noopener">http://www.peixun.net/view/1281.html</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;DecisionTreeClassifier&quot;&gt;&lt;a href=&quot;#DecisionTreeClassifier&quot; class=&quot;headerlink&quot; title=&quot;DecisionTreeClassifier&quot;&gt;&lt;/a&gt;DecisionTreeClassifier&lt;/h1&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;class sklearn.ensemble.RandomForestRegressor(n_estimators=’warn’, criterion=’mse’, &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;max_depth=None, min_samples_split=2, min_samples_leaf=1, &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;min_weight_fraction_leaf=0.0, max_features=’auto’, max_leaf_nodes=None, &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Python機器學習" scheme="http://www.taroballz.com/categories/Python%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92/"/>
    
    
      <category term="Python" scheme="http://www.taroballz.com/tags/Python/"/>
    
      <category term="2019" scheme="http://www.taroballz.com/tags/2019/"/>
    
      <category term="演算法" scheme="http://www.taroballz.com/tags/%E6%BC%94%E7%AE%97%E6%B3%95/"/>
    
      <category term="機器學習(Machine Learning)" scheme="http://www.taroballz.com/tags/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>機器學習-演算法-隨機森林分類(RandomForestClassifier)</title>
    <link href="http://www.taroballz.com/2019/05/25/ML_RandomForest_Classifier/"/>
    <id>http://www.taroballz.com/2019/05/25/ML_RandomForest_Classifier/</id>
    <published>2019-05-25T15:25:00.000Z</published>
    <updated>2019-05-25T15:52:19.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>隨機森林是非常具有代表性的Bagging集成演算法</p><ul><li>所有的基評估器(base estimator)都是決策樹</li><li>單個決策樹的準確率越高，隨機森林的準確率也會越高<ul><li>Bagging是依賴於<strong>平均值</strong>或<strong>多數決原則</strong>來決定集成結果的</li></ul></li></ul><h1 id="DecisionTreeClassifier"><a href="#DecisionTreeClassifier" class="headerlink" title="DecisionTreeClassifier"></a>DecisionTreeClassifier</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">class sklearn.ensemble.RandomForestClassifier(n_estimators=’warn’, criterion=’gini’, </span><br><span class="line">max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, </span><br><span class="line">max_features=’auto’, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, </span><br><span class="line">bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, </span><br><span class="line">class_weight=None)</span><br></pre></td></tr></table></figure><a id="more"></a><ul><li>其大部分的參數幾乎與決策樹一致</li></ul><h1 id="重要參數"><a href="#重要參數" class="headerlink" title="重要參數"></a>重要參數</h1><ul><li>其是使用bagging的集成算法都會有的重要參數</li></ul><h2 id="n-estimators"><a href="#n-estimators" class="headerlink" title="n_estimators"></a>n_estimators</h2><blockquote><p>森林中樹木的數量(base estimator的數量)</p><ul><li>通常此值越大，模型效果往往越好<ul><li>任何模型都有決策邊界，其值達到一定程度時，精確性往往不在上升或開始波動</li><li>其值越大，計算量與內存消耗量越大，訓練時間越長</li><li>渴望在訓練難度和模型效果之間取得平衡</li></ul></li></ul></blockquote><h2 id="random-state"><a href="#random-state" class="headerlink" title="random_state"></a>random_state</h2><ul><li>隨機森林中的<code>random_state</code>，其控制的是森林生成的模式(生成一片固定的森林)，而非讓一個森林之中只有一棵樹<ul><li>固定生成的每棵樹的<code>random_state</code>;但是森林中樹跟樹之間還是不一樣</li></ul></li><li>不像決策樹的<code>random_state</code>都是生成同一棵樹</li></ul><h2 id="bootstrap"><a href="#bootstrap" class="headerlink" title="bootstrap"></a>bootstrap</h2><blockquote><p>bagging是通過樣本抽取後有放回的隨機抽樣技術來型成不同的訓練集<br><code>bootstrap</code>就是用來控制抽樣技術的參數</p></blockquote><ul><li>參數默認為<code>True</code>:有放回的隨機採樣技術<ul><li>通常這個參數不會被我們設置為<code>False</code></li></ul></li><li>由於是有放回的抽樣技術，樣本可能在新的自助集中出現多次，其他樣本可能被忽略</li><li>自助集大約平均會包含 <strong>63%</strong> 的原始數據<ul><li>表示會有37%的數據集被浪費掉，沒有參與建模，其被稱為袋外數據(out of bag data;oob)<ul><li>亦可以被用來作為集成算法的測試集使用</li><li><strong>亦指在使用隨機森林時，我們可以不劃分測試集和訓練集，只需要用袋外數據來測試模型</strong></li><li><strong>當原始樣本數及<code>n_estimators</code>不夠大的時候</strong>，也很可能沒有數據落在袋外，便無法使用oob數據來測試模型</li></ul></li></ul></li></ul><h2 id="oob-score"><a href="#oob-score" class="headerlink" title="oob_score"></a>oob_score</h2><ul><li>若希望用袋外數據測試，實例化時將<code>oob_score</code>設為<code>True</code><ul><li><code>RF_classifier = RandomForestClassifier(n_estimators=25,random_state=2,oob_score=True)</code></li></ul></li><li>為<code>True</code>時無須再劃分訓練集及測試集</li></ul><h1 id="重要屬性"><a href="#重要屬性" class="headerlink" title="重要屬性"></a>重要屬性</h1><h2 id="estimators"><a href="#estimators" class="headerlink" title="estimators_"></a>estimators_</h2><ul><li>使用<code>estimator.estimators_</code>查看森林中所有樹的狀況</li><li>其返回每棵樹對象構建的參數，為一列表<ul><li>可以發現每棵樹只有<code>random_state</code>不同</li></ul></li><li>可使用列表的方式取出某棵樹，再利用<code>.屬性</code>的方式查看該樹的屬性<ul><li><code>estimator.estimators_[0].random_state</code><ul><li>查看<code>estimator</code>模型裡第0顆樹的<code>random_state</code>之值</li></ul></li></ul></li></ul><h2 id="oobscore"><a href="#oobscore" class="headerlink" title="oobscore"></a>oob<em>score</em></h2><ul><li>要是建模<code>oob_score</code>為<code>True</code>時，可使用其屬性查看使用袋外數據測試的結果</li></ul><h1 id="重要接口"><a href="#重要接口" class="headerlink" title="重要接口"></a>重要接口</h1><p>和其他演算法模型一樣，隨機森林仍有<code>apply</code>、<code>fit</code>、<code>predict</code>、<code>score</code>等接口</p><h2 id="predict-proba-predict-probability"><a href="#predict-proba-predict-probability" class="headerlink" title="predict_proba (predict probability)"></a>predict_proba (predict probability)</h2><p>返回每個<strong>測試樣本對應被分到每一類標籤的概率</strong></p><ul><li>標籤有幾種不一樣的分類就會返回幾個概率<ul><li>二分類問題：數值大於0.5被分為1，小於0.5被分為0</li></ul></li></ul><blockquote><p>傳統隨機森林是利用bagging規則，平均或多數決集成結果</p><ul><li><code>sklearn</code>中隨機森林<strong>是平均每個樣本對應的predict_proba返回的概率，得到一個平均概率，從而決定樣本的分類</strong></li></ul></blockquote><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="http://www.peixun.net/view/1281.html" target="_blank" rel="noopener">http://www.peixun.net/view/1281.html</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h1&gt;&lt;p&gt;隨機森林是非常具有代表性的Bagging集成演算法&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;所有的基評估器(base estimator)都是決策樹&lt;/li&gt;
&lt;li&gt;單個決策樹的準確率越高，隨機森林的準確率也會越高&lt;ul&gt;
&lt;li&gt;Bagging是依賴於&lt;strong&gt;平均值&lt;/strong&gt;或&lt;strong&gt;多數決原則&lt;/strong&gt;來決定集成結果的&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;DecisionTreeClassifier&quot;&gt;&lt;a href=&quot;#DecisionTreeClassifier&quot; class=&quot;headerlink&quot; title=&quot;DecisionTreeClassifier&quot;&gt;&lt;/a&gt;DecisionTreeClassifier&lt;/h1&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;class sklearn.ensemble.RandomForestClassifier(n_estimators=’warn’, criterion=’gini’, &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;max_features=’auto’, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;class_weight=None)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Python機器學習" scheme="http://www.taroballz.com/categories/Python%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92/"/>
    
    
      <category term="Python" scheme="http://www.taroballz.com/tags/Python/"/>
    
      <category term="2019" scheme="http://www.taroballz.com/tags/2019/"/>
    
      <category term="演算法" scheme="http://www.taroballz.com/tags/%E6%BC%94%E7%AE%97%E6%B3%95/"/>
    
      <category term="機器學習(Machine Learning)" scheme="http://www.taroballz.com/tags/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>機器學習-集成學習(ensemble learning)方法概述</title>
    <link href="http://www.taroballz.com/2019/05/22/ML_Introduction_EnsembleLearning/"/>
    <id>http://www.taroballz.com/2019/05/22/ML_Introduction_EnsembleLearning/</id>
    <published>2019-05-22T02:25:00.000Z</published>
    <updated>2019-05-25T15:42:22.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="集成學習-ensemble-learning-方法"><a href="#集成學習-ensemble-learning-方法" class="headerlink" title="集成學習(ensemble learning)方法"></a>集成學習(ensemble learning)方法</h1><p>通過<strong>建立幾個模型組合來解決單一預測問題</strong>，其工作原理是在數據集上<strong>構建多個分類器/模型</strong>，各自獨立學習和做出預測，這些預測最後結合成單預測，因此優於任何一個單分類器做出的預測</p><ul><li><strong>不是一個單獨的機器學習的算法阿</strong></li><li>現在各種演算法競賽中，隨機森林、梯度提升樹(GBDT)、Xgboost隨處可見</li><li>sklearn中的集成學習方法位於<code>sklearn.ensemble</code>中</li></ul><a id="more"></a><h1 id="集成演算法"><a href="#集成演算法" class="headerlink" title="集成演算法"></a>集成演算法</h1><ul><li>多個演算法模型集成成為的模型叫做<strong>集成評估器</strong>(ensemble estimator)</li><li>組成集成評估器的每個演算法模型都叫做<strong>基評估器</strong>(base estimator)</li><li>通常分為三類：<ul><li>裝袋法(Bagging)<ul><li>構建多個<strong>相互獨立的評估器</strong>(彼此建立的過程互不干擾)</li><li>對其預測進行<strong>平均</strong>或<strong>多數表決原則</strong>來決定結果</li><li>隨機森林</li><li><blockquote><p>單獨一個基評估器判斷準確率至少要超過50%，才可以確保集成評估器的表現會比基評估器好;否則集成評估器的效果會劣於基評估器。<strong>在使用隨機森林之前，一定要檢查，用來組成隨機森林的分類樹是否都至少有50%的預測正確率</strong></p></blockquote></li></ul></li><li>提升法(Boosting)<ul><li><strong>基評估器是相關的</strong>(是按順序構建的)<ul><li>第一次採樣訓練，對於判斷錯誤的樣本，在下一次採樣訓練模型的過程中，會增加其權重，使其更容易被下一個建立的評估器提取到，不斷循環</li><li>在一次次建模的過程中，對難以評估特徵的樣本進行強力的預測，構建出一個強評估器</li></ul></li><li>Adaboost、梯度提升樹</li></ul></li><li>stacking</li></ul></li></ul><h2 id="集成算法目標"><a href="#集成算法目標" class="headerlink" title="集成算法目標"></a>集成算法目標</h2><p>集成算法會考慮多個estimator建模的結果，匯總後得到一個綜合的結果</p><ul><li><strong>以此來獲取比單個模型更好的回歸或分類表現</strong></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;集成學習-ensemble-learning-方法&quot;&gt;&lt;a href=&quot;#集成學習-ensemble-learning-方法&quot; class=&quot;headerlink&quot; title=&quot;集成學習(ensemble learning)方法&quot;&gt;&lt;/a&gt;集成學習(ensemble learning)方法&lt;/h1&gt;&lt;p&gt;通過&lt;strong&gt;建立幾個模型組合來解決單一預測問題&lt;/strong&gt;，其工作原理是在數據集上&lt;strong&gt;構建多個分類器/模型&lt;/strong&gt;，各自獨立學習和做出預測，這些預測最後結合成單預測，因此優於任何一個單分類器做出的預測&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;不是一個單獨的機器學習的算法阿&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;現在各種演算法競賽中，隨機森林、梯度提升樹(GBDT)、Xgboost隨處可見&lt;/li&gt;
&lt;li&gt;sklearn中的集成學習方法位於&lt;code&gt;sklearn.ensemble&lt;/code&gt;中&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Python機器學習" scheme="http://www.taroballz.com/categories/Python%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92/"/>
    
    
      <category term="Python" scheme="http://www.taroballz.com/tags/Python/"/>
    
      <category term="2019" scheme="http://www.taroballz.com/tags/2019/"/>
    
      <category term="演算法" scheme="http://www.taroballz.com/tags/%E6%BC%94%E7%AE%97%E6%B3%95/"/>
    
      <category term="機器學習(Machine Learning)" scheme="http://www.taroballz.com/tags/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-Machine-Learning/"/>
    
      <category term="集成學習方法" scheme="http://www.taroballz.com/tags/%E9%9B%86%E6%88%90%E5%AD%B8%E7%BF%92%E6%96%B9%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>機器學習-演算法-決策樹回歸樹(DecisionTreeRegressor)</title>
    <link href="http://www.taroballz.com/2019/05/19/ML_decision_tree_TreeRegressor/"/>
    <id>http://www.taroballz.com/2019/05/19/ML_decision_tree_TreeRegressor/</id>
    <published>2019-05-19T06:43:00.000Z</published>
    <updated>2019-05-26T08:16:16.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="DecisionTreeRegressor"><a href="#DecisionTreeRegressor" class="headerlink" title="DecisionTreeRegressor"></a>DecisionTreeRegressor</h1><a id="more"></a><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">class sklearn.tree.DecisionTreeRegressor(criterion=’mse’, splitter=’best’, </span><br><span class="line">max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, </span><br><span class="line">max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, </span><br><span class="line">min_impurity_split=None, presort=False)</span><br></pre></td></tr></table></figure><ul><li>幾乎所有參數、屬性、接口都和分類樹相同</li><li>回歸樹<strong>沒有標籤分佈是否均衡的問題</strong><ul><li>因此沒有<code>class_weight</code>這樣的參數</li></ul></li></ul><h1 id="重要參數"><a href="#重要參數" class="headerlink" title="重要參數"></a>重要參數</h1><h2 id="criterion"><a href="#criterion" class="headerlink" title="criterion"></a>criterion</h2><blockquote><p>回歸樹衡量分枝質量的指標，支持的標準有三種</p></blockquote><ul><li><code>&quot;mse&quot;</code>: 使用均方誤差(mean squared error)<ul><li>父節點和葉子節點之間<strong>均方誤差之差額</strong>被用作特徵選擇的標準</li><li>使用葉子節點的<strong>平均值</strong>來最小化L2損失</li></ul></li><li><code>&quot;friedman_mse&quot;</code>：費爾德曼均方誤差<ul><li>對潛在分枝問題進行改進的另一種計算均方誤差的方法</li></ul></li><li><code>&quot;mae&quot;</code>：使用絕對平均誤差(mean absolute error)<ul><li>使用葉子節點的<strong>中位數</strong>來最小化L1損失    </li></ul></li></ul><h1 id="Mean-Squared-Error-MSE"><a href="#Mean-Squared-Error-MSE" class="headerlink" title="Mean Squared Error(MSE)"></a>Mean Squared Error(MSE)</h1><script type="math/tex; mode=display">MSE = \frac{1}{N} \sum_{i=1}^{N} (f_i - y_i)^2</script><ul><li>$f_i$：是模型回歸出的數值</li><li>$y_i$：是樣本點$i$實際的數值標籤</li></ul><h1 id="重要接口"><a href="#重要接口" class="headerlink" title="重要接口"></a>重要接口</h1><h2 id="score"><a href="#score" class="headerlink" title="score"></a>score</h2><blockquote><p>回歸樹中恆量指標，返回的是$R^2$，並不是MSE</p><ul><li>$R^2$範圍為1至負無窮大</li></ul></blockquote><h2 id="tips"><a href="#tips" class="headerlink" title="tips"></a>tips</h2><ul><li>在回歸樹中,MSE是<strong>分枝質量</strong>衡量指標</li><li>在回歸樹中,MSE是最常使用<strong>回歸質量</strong>衡量指標</li><li>使用交叉驗證(cross_validation)時，通常選擇<strong>均方誤差</strong>作為評估<ul><li><code>cross_val_score(regressor,data,target,cv=10,scoring=&quot;neg_mean_squared_error&quot;)</code><ul><li><code>scoring</code>不指定的話，預設返回的是$R^2$</li></ul></li></ul></li><li>在回歸模型中,MSE<strong>越小越好</strong></li></ul><h1 id="結論"><a href="#結論" class="headerlink" title="結論"></a>結論</h1><p>如果樹的最大深度(<code>max_depth</code>)設置得太高，決策樹會學習的太精細</p><ul><li>從訓練數據中學了太多細節(包括noise)</li><li>使模型偏離真實欲表達得曲線，變成<strong>過擬合</strong></li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="http://www.peixun.net/view/1281.html" target="_blank" rel="noopener">http://www.peixun.net/view/1281.html</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;DecisionTreeRegressor&quot;&gt;&lt;a href=&quot;#DecisionTreeRegressor&quot; class=&quot;headerlink&quot; title=&quot;DecisionTreeRegressor&quot;&gt;&lt;/a&gt;DecisionTreeRegressor&lt;/h1&gt;
    
    </summary>
    
      <category term="Python機器學習" scheme="http://www.taroballz.com/categories/Python%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92/"/>
    
    
      <category term="Python" scheme="http://www.taroballz.com/tags/Python/"/>
    
      <category term="2019" scheme="http://www.taroballz.com/tags/2019/"/>
    
      <category term="演算法" scheme="http://www.taroballz.com/tags/%E6%BC%94%E7%AE%97%E6%B3%95/"/>
    
      <category term="機器學習(Machine Learning)" scheme="http://www.taroballz.com/tags/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>機器學習-演算法-細談決策樹分類樹(DecisionTreeClassifier)</title>
    <link href="http://www.taroballz.com/2019/05/15/ML_decision_tree_detail/"/>
    <id>http://www.taroballz.com/2019/05/15/ML_decision_tree_detail/</id>
    <published>2019-05-15T13:25:00.000Z</published>
    <updated>2019-05-19T06:39:38.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><ul><li>一種<strong>非參數</strong>的<strong>監督學習(有目標值)</strong>的演算法<ul><li>非參數：不限制數據的結構與類型<ul><li>任何數據皆適用<a id="more"></a></li></ul></li></ul></li><li>只要是決策樹的葉子節點(有進邊，沒有出邊)，都是一個<strong>類別的標籤</strong></li><li>決策樹演算法的核心是要解決<strong>兩個問題</strong><ul><li>如何從數據中找出<strong>最佳節點</strong>和<strong>最佳分枝</strong></li><li>如何讓決策樹停止生長，防止<strong>過擬合</strong></li></ul></li></ul><h1 id="sklearn中的決策樹"><a href="#sklearn中的決策樹" class="headerlink" title="sklearn中的決策樹"></a>sklearn中的決策樹</h1><ul><li>sklearn中關於決策樹的類(不包含集成演算法)都在<code>sklearn.tree</code>這個模塊下，共包含五個類<ul><li><code>tree.DecisionTreeClassifier</code>：分類樹</li><li><code>tree.DecisionTreeRegressor</code>：回歸樹</li><li><code>tree.export_graphviz</code>：將生成的決策樹導出為DOT格式，畫圖專用</li><li><code>tree.ExtraTreeClassifier</code>：<strong>高隨機</strong>版本的分類樹</li><li><code>tree.ExtraTreeRegressor</code>：<strong>高隨機</strong>版本的回歸樹</li></ul></li></ul><h1 id="DecisionTreeClassifier"><a href="#DecisionTreeClassifier" class="headerlink" title="DecisionTreeClassifier"></a>DecisionTreeClassifier</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">class sklearn.tree.DecisionTreeClassifier (criterion=’gini’, splitter=’best’, max_depth=None, </span><br><span class="line">min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, </span><br><span class="line">max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort=False)</span><br></pre></td></tr></table></figure><h2 id="重要參數"><a href="#重要參數" class="headerlink" title="重要參數"></a>重要參數</h2><h3 id="criterion"><a href="#criterion" class="headerlink" title="criterion"></a>criterion</h3><blockquote><p>對分類樹來說，找出最佳節點及分枝的指標為＂不純度＂，<strong>不純度越低，決策樹對訓練集得的擬合越好</strong>。<br>決策樹就是追求不純度相關指標的最優化</p><ul><li>樹中的每個節點都會有一個不純度<ul><li>子節點的不純度定低於父節點</li><li>葉子節點的不純度是最低的</li></ul></li></ul></blockquote><ul><li><code>&quot;entropy&quot;</code>: 使用信息熵<ul><li>信息熵對不純度叫基尼係數強(對不純度懲罰最強)<ul><li>容易過擬合</li></ul></li><li><strong>實際使用中兩者效果基本一樣</strong></li></ul></li><li><code>&quot;gini&quot;</code>：使用基尼係數<ul><li>適用於數據維度大，噪音很大的數據</li></ul></li></ul><h2 id="隨機性參數"><a href="#隨機性參數" class="headerlink" title="隨機性參數"></a>隨機性參數</h2><h3 id="random-state"><a href="#random-state" class="headerlink" title="random_state"></a>random_state</h3><blockquote><p>設置分枝中隨機模式的參數，預設為None<br>使用同一批訓練集(<code>Xtrain</code>)進行訓練，得到的準確度仍有差異，原因決策樹<strong>本身就具有隨機性</strong><br><strong>輸入任意整數</strong>，會針對特定模型選定的feature,長出同一顆樹讓模型穩定下來(固定隨機性)</p><ul><li>調參優化時通常會固定random_state</li></ul></blockquote><h2 id="splitter"><a href="#splitter" class="headerlink" title="splitter"></a>splitter</h2><blockquote><p>用作控制決策樹的隨機性</p></blockquote><ul><li><code>&quot;best&quot;</code>:<ul><li>決策樹雖然本身就具有隨機性，但會<strong>優先選擇更重要的特徵進行分枝</strong></li></ul></li><li><code>&quot;random</code>:<ul><li>決策樹在分枝時會更為隨機<ul><li>樹深度更深</li><li>對訓練集的擬合會降低(<strong>防止過擬合的一種方式</strong>)</li></ul></li></ul></li></ul><h2 id="剪枝參數"><a href="#剪枝參數" class="headerlink" title="剪枝參數"></a>剪枝參數</h2><blockquote><p>決策樹會生長到<strong>衡量不純度最優</strong>，或者沒有更多特徵可用為止</p><ul><li>這樣決策樹往往會過擬合</li><li><strong>會在訓練集上表現很好，測試集卻很糟糕</strong></li><li>為了決策樹有更好的泛化性，須對決策樹進行剪枝<ul><li><strong>剪枝策略對決策樹的影響巨大，正確的剪枝策略是優化決策樹算法的核心</strong></li></ul></li></ul></blockquote><h3 id="max-depth"><a href="#max-depth" class="headerlink" title="max_depth"></a>max_depth</h3><blockquote><p>限制樹的最大深度，超過設定深度的樹枝全部剪掉</p><ul><li>用的最廣泛的剪枝參數</li><li>在<strong>高維度、低樣本量時</strong>非常有效</li><li>在集成算法中也非常實用</li></ul></blockquote><ul><li>建議從 <code>= 3</code>開始嘗試</li></ul><h3 id="min-samples-leaf"><a href="#min-samples-leaf" class="headerlink" title="min_samples_leaf"></a>min_samples_leaf</h3><blockquote><p>一個節點在分枝後每個子節點都必須包含至少<code>min_samples_leaf</code>個<strong>訓練樣本</strong>；<br>分枝會朝著滿足每個子節點都包含<code>min_samples_leaf</code>個樣本的方向去發展</p><ul><li>一般搭配<code>max_depth</code>使用</li><li>一般建議從<code>= 5</code>開始使用</li><li>如果葉節點中含有的樣本量變化很大，<strong>建議輸入浮點樹作為樣本量的百分比使用</strong></li></ul></blockquote><h3 id="min-samples-split"><a href="#min-samples-split" class="headerlink" title="min_samples_split"></a>min_samples_split</h3><blockquote><p>一個<strong>中間節點</strong>必須包含至少<code>min_samples_split</code>個訓練樣本，這個節點在允許被分枝，否則分枝不會發生</p></blockquote><h3 id="max-features"><a href="#max-features" class="headerlink" title="max_features"></a>max_features</h3><blockquote><p>限制分枝時考慮的特徵個數，超過限制個數的特徵都會被捨棄<br>和<code>max_depth</code>異曲同工</p><ul><li>在不知道決策樹中的各個特徵的重要性情況下，設定這個參數會導致學習不足</li></ul></blockquote><h3 id="min-impurity-decrease"><a href="#min-impurity-decrease" class="headerlink" title="min_impurity_decrease"></a>min_impurity_decrease</h3><blockquote><p>限制信息增益的大小<br>信息增益小於設定數值的分枝不會發生</p></blockquote><h3 id="利用matplotlib畫出學習曲線來確認最優的剪枝參數"><a href="#利用matplotlib畫出學習曲線來確認最優的剪枝參數" class="headerlink" title="利用matplotlib畫出學習曲線來確認最優的剪枝參數"></a>利用matplotlib畫出學習曲線來確認最優的剪枝參數</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">y_eff = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):   <span class="comment"># 測試的條件數</span></span><br><span class="line">    tree_clf = tree.DecisionTreeClassifier(criterion=<span class="string">"entropy"</span></span><br><span class="line">                                          ,random_state = <span class="number">30</span></span><br><span class="line">                                          ,splitter = <span class="string">"random"</span></span><br><span class="line">                                          ,max_depth = i+<span class="number">1</span>   <span class="comment">#測試條件</span></span><br><span class="line">                                          )</span><br><span class="line">  tree_clf = tree_clf.fit(Xtrain,ytrain)</span><br><span class="line">  score = tree_clf.score(Xtest,ytest)</span><br><span class="line">  y_eff.append(score)</span><br><span class="line">plt.plot(range(<span class="number">1</span>,<span class="number">11</span>),y_eff,color=<span class="string">"red"</span>,label=<span class="string">"max_depth"</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="目標權重參數"><a href="#目標權重參數" class="headerlink" title="目標權重參數"></a>目標權重參數</h2><p>控制目標權重的參數</p><h3 id="class-weight"><a href="#class-weight" class="headerlink" title="class_weight"></a>class_weight</h3><blockquote><p>樣本不平衡是指在一組數據集中，某個標籤天生佔有很大的比例<br>完成樣本標籤平衡的參數，對樣本標籤進行一定的均衡</p><ul><li>給少量的標籤更多的權重</li><li>讓模型更偏向少數類，向捕獲少數類的方向建模</li></ul></blockquote><ul><li>該參數默認為<code>None</code><ul><li>自動給予數據集中的所有標籤相同的權重</li></ul></li></ul><h3 id="min-weight-fraction-leaf"><a href="#min-weight-fraction-leaf" class="headerlink" title="min_weight_fraction_leaf"></a>min_weight_fraction_leaf</h3><blockquote><p>有了權重之後，樣本量就不再是單純記錄樹目，而是受輸入權重影響<br>此時剪枝須搭配<code>min_weight_fraction_leaf</code>這個基於權重的剪枝參數使用</p></blockquote><h1 id="重要屬性和接口"><a href="#重要屬性和接口" class="headerlink" title="重要屬性和接口"></a>重要屬性和接口</h1><p>所有接口中要求輸入<code>X_train,X_test</code>的部分，輸入的特徵矩陣必須至少是一個<strong>二維</strong>的矩陣</p><ul><li>sklearn不接受任何一維矩陣作為特徵矩陣被輸入<ul><li>如果數據確實<strong>只有一個特徵</strong>的data<ul><li>必須用<code>reshape(-1,1)</code>來給矩陣增維</li></ul></li><li>如果數據<strong>只有一個特徵和一個樣本</strong>(one-sample)<ul><li>必須用<code>reshape(1,-1)</code>來給數據增維</li></ul></li></ul></li></ul><h2 id="estimator-apply-Xtest"><a href="#estimator-apply-Xtest" class="headerlink" title="estimator.apply(Xtest)"></a>estimator.apply(Xtest)</h2><p>返回每個<strong>測試樣本所在的葉子節點的索引</strong></p><ul><li>只輸入測試集，返回預測的結果</li></ul><h2 id="estimator-predict-Xtest"><a href="#estimator-predict-Xtest" class="headerlink" title="estimator.predict(Xtest)"></a>estimator.predict(Xtest)</h2><p>返回每個<strong>測試樣本分類/回歸的結果</strong></p><ul><li>只輸入測試集，返回預測的結果</li></ul><h1 id="結論"><a href="#結論" class="headerlink" title="結論"></a>結論</h1><p>分類樹(DecisionTreeClassifier)天生不擅長環形數據，每個模型都有自己決策的上限</p><ul><li>一個怎樣調整參數都無法提升的表現的可能性仍存在</li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="http://www.peixun.net/view/1281.html" target="_blank" rel="noopener">http://www.peixun.net/view/1281.html</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;一種&lt;strong&gt;非參數&lt;/strong&gt;的&lt;strong&gt;監督學習(有目標值)&lt;/strong&gt;的演算法&lt;ul&gt;
&lt;li&gt;非參數：不限制數據的結構與類型&lt;ul&gt;
&lt;li&gt;任何數據皆適用
    
    </summary>
    
      <category term="Python機器學習" scheme="http://www.taroballz.com/categories/Python%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92/"/>
    
    
      <category term="Python" scheme="http://www.taroballz.com/tags/Python/"/>
    
      <category term="2019" scheme="http://www.taroballz.com/tags/2019/"/>
    
      <category term="演算法" scheme="http://www.taroballz.com/tags/%E6%BC%94%E7%AE%97%E6%B3%95/"/>
    
      <category term="機器學習(Machine Learning)" scheme="http://www.taroballz.com/tags/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-Machine-Learning/"/>
    
  </entry>
  
</feed>
